{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandboxRL: DDQN Atari | EPOCH Laboratory 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "import gym\n",
    "import os, datetime, imageio, cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.4.1\n",
      "Eager mode: True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(\"Version:\", tf.__version__)\n",
    "print(\"Eager mode:\", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if physical_devices else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(gym.envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/gym/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 289714752]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame: Box(84, 84, 4)\n",
      "States: 84\n",
      "Actions: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_space = env.observation_space\n",
    "num_states = env.observation_space.shape\n",
    "action_space = env.action_space.n\n",
    "\n",
    "print('Frame:', screen_space)\n",
    "print('States:', num_states[0])\n",
    "print('Actions:', action_space)\n",
    "\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of environment\n",
    "#env.reset()\n",
    "#for i in range(1000):\n",
    "#    env.render()\n",
    "#    action = np.random.choice(action_space)\n",
    "#    new_frame, reward, terminal, info = env.step(action)\n",
    "#    #print(reward, terminal, info['ale.lives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32                                 # Size of batch taken from replay buffer\n",
    "MAX_STEPS_PER_EPISODE = 18000                   # 5mins at 60fps = 18000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY_LENGTH = 1000000                     # Maximum replay length\n",
    "UPDATE_AFTER_ACTIONS = 4                        # Train the model after 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_RANDOM_FRAMES = 50000                   # Number of frames to take random action and observe output\n",
    "EPSILON_GREEDY_FRAMES = 1000000.0               # Number of frames for exploration\n",
    "\n",
    "GAMMA = 0.99                                    # Discount factor for past rewards\n",
    "EPSILON = 1.0                                   # Epsilon greedy parameter\n",
    "EPSILON_MIN = 0.1                               # Minimum epsilon greedy parameter\n",
    "EPSILON_MAX = 1.0                               # Maximum epsilon greedy parameter\n",
    "EPSILON_ANNEALER = (EPSILON_MAX - EPSILON_MIN)  # Rate at which to reduce chance of random action being taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_TARGET_NETWORK = 10000                   # How often to update the target network\n",
    "TAU = 0.08                                      # Dynamic update factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE = True                                   # Double DQN\n",
    "DEULING = True                                  # Deuling DQN\n",
    "PLAYBACK = False                                # Vizualize Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create convolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_model():\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(layer4)\n",
    "    \n",
    "    if DEULING:\n",
    "        value = layers.Dense(1, kernel_initializer='he_uniform')(layer5)\n",
    "        value = layers.Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(value)\n",
    "        \n",
    "        adv = layers.Dense(action_space, kernel_initializer='he_uniform')(layer5)\n",
    "        adv = layers.Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(adv)\n",
    "        \n",
    "        action = layers.Add()([value, adv])\n",
    "    else:\n",
    "        action = layers.Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 84, 84, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 20, 20, 32)   8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9, 9, 64)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1606144     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            3078        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 6)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 6)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,687,719\n",
      "Trainable params: 1,687,719\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The first model makes the predictions for Q-values which are used to make a action.\n",
    "model = q_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a target model for the prediction of future rewards.\n",
    "model_target = q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "#optimizer = keras.optimizers.RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01 / 32**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture(step, sequence):\n",
    "    if step < 600:\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        sequence.append(frame)\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration(eps, nstate, step):\n",
    "    if frame_count < EPSILON_RANDOM_FRAMES or eps > np.random.rand(1)[0]:\n",
    "        action = np.random.choice(action_space)\n",
    "    else:\n",
    "        state_tensor = tf.convert_to_tensor(nstate)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = model(state_tensor, training=False)\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "    \n",
    "    # NOOP - Fire on first frame of episode\n",
    "    if step == 0:\n",
    "        action = 1\n",
    "        \n",
    "    eps -= EPSILON_ANNEALER / EPSILON_GREEDY_FRAMES\n",
    "    eps = max(eps, EPSILON_MIN)\n",
    "    \n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(naction):\n",
    "    state_next, reward, terminal, info = env.step(naction)\n",
    "    state_next = np.array(state_next)\n",
    "    \n",
    "    return state_next, reward, terminal, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punish(health, feedback):\n",
    "    if info['ale.lives'] < health:\n",
    "        life_lost = True\n",
    "    else:\n",
    "        life_lost = feedback\n",
    "    health = info['ale.lives']\n",
    "    \n",
    "    return life_lost, health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory(naction, nstate, nstate_next, nterminal, nreward):\n",
    "    action_history.append(naction)\n",
    "    state_history.append(nstate)\n",
    "    state_next_history.append(nstate_next)\n",
    "    terminal_history.append(nterminal)\n",
    "    rewards_history.append(nreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(memory):\n",
    "    indices = np.random.choice(range(len(memory)), size=BATCH_SIZE)\n",
    "\n",
    "    state_sample = np.array([state_history[i] for i in indices])\n",
    "    state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "    rewards_sample = [rewards_history[i] for i in indices]\n",
    "    action_sample = [action_history[i] for i in indices]\n",
    "    terminal_sample = tf.convert_to_tensor([float(memory[i]) for i in indices])\n",
    "    \n",
    "    return state_sample, state_next_sample, rewards_sample, action_sample, terminal_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target(target_weights, weights, ratio, dynamic):\n",
    "    if dynamic:\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * ratio + a * (1 - ratio))\n",
    "    else:\n",
    "        if frame_count % UPDATE_TARGET_NETWORK == 0:\n",
    "            model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(episode_id, instance):\n",
    "    state = np.array(env.reset())\n",
    "    \n",
    "    episode_reward = 0\n",
    "    frames = []\n",
    "\n",
    "    for timestep in range(1, MAX_STEPS_PER_EPISODE):\n",
    "\n",
    "        # Capture gameplay experience\n",
    "        frames = capture(timestep, frames)\n",
    "\n",
    "        # Predict action Q-values from environment state and take best action\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = model(state_tensor, training=False)\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, terminal, _ = env.step(action)\n",
    "        state = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "    render_gif(frames, log_dir + timestamp + \"/\" + instance + \"_\" + str(episode_id) + \"_\" + str(episode_reward))\n",
    "\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gif(frames, filename):   \n",
    "    return imageio.mimsave(filename + '.gif', frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 20210705-200405\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"ID:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\"\n",
    "summary_writer = tf.summary.create_file_writer(log_dir + timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tensorboard --logdir=\" + str(log_dir) + \" --port=6006 &\"\n",
    "os.system(command)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model)\n",
    "checkpoint_path = \"training_checkpoints/\" + timestamp + \"/training_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "terminal_history = []\n",
    "episode_reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_reward = -21\n",
    "eval_reward = -21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -20.37 at episode 19, frame count 18000\n",
      "running reward: -20.18 at episode 38, frame count 36000\n",
      "running reward: -20.26 at episode 57, frame count 54000\n",
      "running reward: -20.23 at episode 77, frame count 72000\n",
      "running reward: -20.19 at episode 95, frame count 90000\n",
      "running reward: -20.11 at episode 113, frame count 108000\n",
      "running reward: -20.06 at episode 130, frame count 126000\n",
      "running reward: -20.00 at episode 146, frame count 144000\n",
      "running reward: -19.86 at episode 163, frame count 162000\n",
      "running reward: -19.75 at episode 179, frame count 180000\n",
      "running reward: -19.67 at episode 195, frame count 198000\n",
      "running reward: -19.59 at episode 211, frame count 216000\n",
      "running reward: -19.48 at episode 226, frame count 234000\n",
      "running reward: -19.35 at episode 240, frame count 252000\n",
      "running reward: -19.32 at episode 257, frame count 270000\n",
      "running reward: -19.24 at episode 272, frame count 288000\n",
      "running reward: -19.20 at episode 286, frame count 306000\n",
      "running reward: -19.18 at episode 301, frame count 324000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f9e4e22eb603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Double Q-Learning, decoupling selection and evaluation of the action seletion with the current DQN model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \"\"\"\n\u001b[0;32m-> 1837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4284\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4285\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \"\"\"\n\u001b[1;32m    337\u001b[0m       \u001b[0mnum_in_full_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_full_batches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m       \u001b[0mfirst_k_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[1;32m    340\u001b[0m           first_k_indices, [num_full_batches, batch_size])\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9389\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9390\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 9391\u001b[0;31m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[1;32m   9392\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9393\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m       default_type_attr_map[key] = dtypes.as_dtype(\n\u001b[1;32m    368\u001b[0m           attr_def.default_value.type)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:  # Run until solved\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "    \n",
    "    terminal_life_lost = True\n",
    "    life = 0\n",
    "\n",
    "    for timestep in range(1, MAX_STEPS_PER_EPISODE):\n",
    "        \n",
    "        if PLAYBACK:\n",
    "            env.render();                                                    # View training in real-time\n",
    "        \n",
    "        frame_count += 1\n",
    "                    \n",
    "        action, EPSILON = exploration(EPSILON, state, timestep)              # Use epsilon-greedy for exploration\n",
    "\n",
    "        state_next, reward, terminal, info = step(action)                    # Apply the sampled action in our environment       \n",
    "        terminal_life_lost, life = punish(life, terminal)                    # Punishment for points lost within before terminal state\n",
    "        \n",
    "        add_memory(action, state, state_next, terminal_life_lost, reward)    # Save actions and states in replay buffer\n",
    "        \n",
    "        episode_reward += reward                                             # Update running reward\n",
    "        state = state_next                                                   # Update state\n",
    "\n",
    "        #############\n",
    "        ### TRAIN ###\n",
    "        #############\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % UPDATE_AFTER_ACTIONS == 0 and len(terminal_history) > BATCH_SIZE:\n",
    "\n",
    "            # Sample from replay buffer\n",
    "            state_sample, state_next_sample, rewards_sample, action_sample, terminal_sample = sample(terminal_history)\n",
    "            \n",
    "            # Double Q-Learning, decoupling selection and evaluation of the action seletion with the current DQN model.\n",
    "            q = model.predict(state_next_sample)\n",
    "            target_q = model_target.predict(state_next_sample)\n",
    "            \n",
    "            # Build the updated Q-values for the sampled future states - DQN / DDQN\n",
    "            if DOUBLE:\n",
    "                max_q = tf.argmax(q, axis=1)\n",
    "                max_actions = tf.one_hot(max_q, action_space)\n",
    "                q_samp = rewards_sample + GAMMA * tf.reduce_sum(tf.multiply(target_q, max_actions), axis=1)\n",
    "            else:\n",
    "                q_samp = rewards_sample + GAMMA * tf.reduce_max(target_q, axis=1)        # Bellman Equation\n",
    "          \n",
    "            q_samp = q_samp * (1 - terminal_sample) - terminal_sample                    # If final frame set the last value to -1\n",
    "            masks = tf.one_hot(action_sample, action_space)                              # Create a mask so we only calculate loss on the updated Q-values\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)                                           # Train the model on the states and updated Q-values\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)           # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                loss = keras.losses.Huber()(q_samp, q_action)                              # Calculate loss between new Q-value and old Q-value\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Update the the target network with new weights\n",
    "        update_target(model_target.trainable_variables, model.trainable_variables, TAU, True)\n",
    "        \n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > MAX_MEMORY_LENGTH:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del terminal_history[:1]\n",
    "\n",
    "        # Log details\n",
    "        if frame_count % MAX_STEPS_PER_EPISODE == 0:\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "            \n",
    "    ############\n",
    "    ### TEST ###\n",
    "    ############\n",
    "    # If running_reward has improved by factor of N; evalute & render without epsilon annealer.\n",
    "    if running_reward > min_reward:  \n",
    "        checkpoint.save(checkpoint_path)\n",
    "        eval_reward = evaluate(episode_count, \"pong_DQN_test\")\n",
    "        min_reward = running_reward + 1\n",
    "    \n",
    "    # Callbacks\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('running_reward', running_reward, step=episode_count)\n",
    "        tf.summary.scalar('eval_reward', eval_reward, step=episode_count)\n",
    "\n",
    "    # Condition to consider the task solved (Pong = 21 | Breakout = 40)\n",
    "    if running_reward == 21:\n",
    "        checkpoint.save(checkpoint_path)\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "        \n",
    "    episode_count += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate training and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = tf.train.latest_checkpoint(\"training_checkpoints/20210608-024241/training_checkpoints-28.index\")\n",
    "\n",
    "#latest = tf.train.latest_checkpoint(checkpoint_path)\n",
    "checkpoint.restore(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: Reward: -12.0\n",
      "Episode 2: Reward: -19.0\n",
      "Episode 3: Reward: -20.0\n",
      "Episode 4: Reward: -18.0\n",
      "Episode 5: Reward: -15.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing for \" + str(nb_episodes) + \" episodes ...\")\n",
    "\n",
    "episode_reward = 0\n",
    "episode_count = 0\n",
    "for i in range(nb_episodes):\n",
    "    episode_reward = evaluate(episode_count, \"pong_DQN_infer\")\n",
    "    print(\"Episode \" + str(episode_count+1) + \": Reward: \" + str(episode_reward))\n",
    "    episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "dimensions = (210, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewslice(state, count):\n",
    "    frame = np.array(state)\n",
    "    frame = processed_frame = np.repeat(frame[:, :, count, np.newaxis], 3, axis=2)\n",
    "    frame = cv2.resize(frame, dsize=(dimensions[1], dimensions[0]))\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(frame, model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer = model.get_layer('conv2d_2')\n",
    "        iterate = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output])\n",
    "        model_out, last_conv_layer = iterate(frame[np.newaxis, :, :, :])\n",
    "        class_out = model_out[:, np.argmax(model_out[0])]\n",
    "        grads = tape.gradient(class_out, last_conv_layer)\n",
    "        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    heatmap = heatmap.reshape((7, 7))\n",
    "    heatmap = cv2.resize(heatmap, (INPUT_SHAPE[1], INPUT_SHAPE[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET) / 255\n",
    "    \n",
    "    processed_frame = np.repeat(frame[:, :, 3, np.newaxis], 3, axis=2)\n",
    "    combined_frame = cv2.resize(heatmap * processed_frame, dsize=(dimensions[1], dimensions[0]))\n",
    "\n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 2 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-41c7bd9d8a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnplot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviewslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnplot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c69ef6d1ef65>\u001b[0m in \u001b[0;36mviewslice\u001b[0;34m(state, count)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviewslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 2 with size 4"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 4\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "for count in range(1,nplot+1):\n",
    "    frame = viewslice(state, count)\n",
    "    ax = fig.add_subplot(1,nplot+1,count)\n",
    "    ax.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9528480bd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(generate_heatmap(state, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "min_q_val = -0.1\n",
    "max_q_val = 0.1\n",
    "\n",
    "action_titles = []\n",
    "q_vals = [0]*env.action_space.n\n",
    "\n",
    "for i, (q_val, label) in enumerate(zip(q_vals[::-1], action_titles[::-1])):\n",
    "    if qval > max_q_val:\n",
    "        max_q_val = q_val\n",
    "    elif q_val < min_q_val:\n",
    "        min_q_val = q_val\n",
    "    \n",
    "print(max_q_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "dpi_res = min(84, 84) / 10\n",
    "fig = plt.Figure((500 / dpi_res, 230 / dpi_res), dpi=dpi_res)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Set up plot\n",
    "ax.set_title('Estimated Value over Time', fontsize=20)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_ylabel('V(s)')\n",
    "ax.plot(values[max(len(values)-200, 0):])  # plot values\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
