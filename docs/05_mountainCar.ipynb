{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duelling-DQN: CartPole-v0 | EPOCH Lab\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "- Python: 3.6.12\n",
    "- Keras-GPU: 2.3.1\n",
    "- KerasRL2: 1.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build OpenAI Gym Environment\n",
    "\n",
    "Get the environment and extract the number of states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/gym/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[123]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 2\n",
      "Actions: 3\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "print('States:', states[0])\n",
    "print('Actions:', actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deep Learning Model\n",
    "\n",
    "Build a very simple model regardless of the dueling architecture if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically. Also, you can build a dueling network by yourself and turn off the dueling network in DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (1, ) + states))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 10,819\n",
      "Trainable params: 10,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure and compile our agent. You can use every built-in tensorflow.keras optimizer and even the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "#duel_dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=10, enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/james/anaconda3/envs/gym/lib/python3.7/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200/150000: episode: 1, duration: 4.311s, episode steps: 200, steps per second:  46, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.061807, mae: 0.814149, mean_q: -1.055888\n",
      "    400/150000: episode: 2, duration: 3.830s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.006198, mae: 1.768291, mean_q: -2.607878\n",
      "    600/150000: episode: 3, duration: 3.803s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.026289, mae: 2.863523, mean_q: -4.214609\n",
      "    800/150000: episode: 4, duration: 3.896s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.050789, mae: 3.993084, mean_q: -5.883262\n",
      "   1000/150000: episode: 5, duration: 3.807s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.100860, mae: 5.111343, mean_q: -7.515832\n",
      "   1200/150000: episode: 6, duration: 3.787s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.154212, mae: 6.205967, mean_q: -9.135663\n",
      "   1400/150000: episode: 7, duration: 3.910s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.246688, mae: 7.256346, mean_q: -10.681918\n",
      "   1600/150000: episode: 8, duration: 3.871s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.303690, mae: 8.248911, mean_q: -12.147234\n",
      "   1800/150000: episode: 9, duration: 3.878s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.466280, mae: 9.230424, mean_q: -13.587441\n",
      "   2000/150000: episode: 10, duration: 3.799s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.496564, mae: 10.170528, mean_q: -15.011138\n",
      "   2200/150000: episode: 11, duration: 3.931s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.466030, mae: 11.085350, mean_q: -16.365353\n",
      "   2400/150000: episode: 12, duration: 3.940s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.603953, mae: 11.999146, mean_q: -17.687006\n",
      "   2600/150000: episode: 13, duration: 3.804s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.698145, mae: 12.807801, mean_q: -18.854935\n",
      "   2800/150000: episode: 14, duration: 3.750s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.805472, mae: 13.659737, mean_q: -20.189983\n",
      "   3000/150000: episode: 15, duration: 3.625s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.218936, mae: 14.464033, mean_q: -21.311289\n",
      "   3200/150000: episode: 16, duration: 3.781s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 1.026629, mae: 15.194703, mean_q: -22.400774\n",
      "   3400/150000: episode: 17, duration: 3.611s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 1.780067, mae: 15.869515, mean_q: -23.318760\n",
      "   3600/150000: episode: 18, duration: 3.782s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.526117, mae: 16.513313, mean_q: -24.317554\n",
      "   3800/150000: episode: 19, duration: 3.832s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 1.154853, mae: 17.216917, mean_q: -25.484581\n",
      "   4000/150000: episode: 20, duration: 3.737s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.721713, mae: 17.916872, mean_q: -26.445484\n",
      "   4200/150000: episode: 21, duration: 3.731s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 1.709658, mae: 18.562149, mean_q: -27.421999\n",
      "   4400/150000: episode: 22, duration: 3.662s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 1.635262, mae: 19.246624, mean_q: -28.388544\n",
      "   4600/150000: episode: 23, duration: 3.714s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.765562, mae: 19.862358, mean_q: -29.352877\n",
      "   4800/150000: episode: 24, duration: 3.711s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.391104, mae: 20.476652, mean_q: -30.159836\n",
      "   5000/150000: episode: 25, duration: 3.728s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.395928, mae: 21.011187, mean_q: -30.986588\n",
      "   5200/150000: episode: 26, duration: 3.757s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.734995, mae: 21.574913, mean_q: -31.970240\n",
      "   5400/150000: episode: 27, duration: 3.736s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 2.215085, mae: 22.196003, mean_q: -32.843903\n",
      "   5600/150000: episode: 28, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 2.469703, mae: 22.773232, mean_q: -33.664062\n",
      "   5800/150000: episode: 29, duration: 3.954s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.607579, mae: 23.247665, mean_q: -34.345680\n",
      "   6000/150000: episode: 30, duration: 3.790s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.711060, mae: 23.698269, mean_q: -34.958496\n",
      "   6200/150000: episode: 31, duration: 3.866s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.491238, mae: 24.201820, mean_q: -35.816837\n",
      "   6400/150000: episode: 32, duration: 3.771s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.808266, mae: 24.695816, mean_q: -36.453793\n",
      "   6600/150000: episode: 33, duration: 3.740s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 3.970582, mae: 25.077549, mean_q: -37.051258\n",
      "   6800/150000: episode: 34, duration: 3.824s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.650395, mae: 25.484726, mean_q: -37.623989\n",
      "   7000/150000: episode: 35, duration: 3.748s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 3.818494, mae: 25.817598, mean_q: -38.122314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7200/150000: episode: 36, duration: 3.760s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.302649, mae: 26.229649, mean_q: -38.808105\n",
      "   7400/150000: episode: 37, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.956753, mae: 26.641050, mean_q: -39.466808\n",
      "   7600/150000: episode: 38, duration: 3.522s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.120373, mae: 27.049522, mean_q: -39.955265\n",
      "   7800/150000: episode: 39, duration: 3.638s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 3.097409, mae: 27.403852, mean_q: -40.536224\n",
      "   8000/150000: episode: 40, duration: 3.717s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.416861, mae: 27.790014, mean_q: -41.029415\n",
      "   8200/150000: episode: 41, duration: 3.665s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 3.925686, mae: 28.179653, mean_q: -41.676319\n",
      "   8400/150000: episode: 42, duration: 3.736s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 4.965041, mae: 28.572329, mean_q: -42.206230\n",
      "   8600/150000: episode: 43, duration: 3.727s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 4.332191, mae: 28.828167, mean_q: -42.584278\n",
      "   8800/150000: episode: 44, duration: 3.692s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 5.497395, mae: 29.110085, mean_q: -42.969162\n",
      "   9000/150000: episode: 45, duration: 3.583s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 4.100895, mae: 29.358265, mean_q: -43.418243\n",
      "   9200/150000: episode: 46, duration: 3.492s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 3.923524, mae: 29.683428, mean_q: -43.941788\n",
      "   9400/150000: episode: 47, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 4.575293, mae: 29.926630, mean_q: -44.253906\n",
      "   9600/150000: episode: 48, duration: 3.659s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 3.316306, mae: 30.224585, mean_q: -44.834774\n",
      "   9800/150000: episode: 49, duration: 3.523s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 4.039244, mae: 30.637138, mean_q: -45.336006\n",
      "  10000/150000: episode: 50, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.495815, mae: 30.979370, mean_q: -45.890831\n",
      "  10200/150000: episode: 51, duration: 3.445s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.882014, mae: 31.333759, mean_q: -46.455956\n",
      "  10400/150000: episode: 52, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 5.823428, mae: 31.670433, mean_q: -46.739491\n",
      "  10600/150000: episode: 53, duration: 3.574s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 4.900941, mae: 31.968000, mean_q: -47.236008\n",
      "  10800/150000: episode: 54, duration: 3.645s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 4.647868, mae: 32.210556, mean_q: -47.700809\n",
      "  11000/150000: episode: 55, duration: 3.726s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 5.228430, mae: 32.479355, mean_q: -48.053566\n",
      "  11200/150000: episode: 56, duration: 3.640s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.935732, mae: 32.766747, mean_q: -48.513729\n",
      "  11400/150000: episode: 57, duration: 3.665s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.617084, mae: 33.045071, mean_q: -48.968750\n",
      "  11600/150000: episode: 58, duration: 3.589s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.222663, mae: 33.309048, mean_q: -49.400867\n",
      "  11800/150000: episode: 59, duration: 3.598s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 5.086204, mae: 33.651184, mean_q: -49.871563\n",
      "  12000/150000: episode: 60, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.222878, mae: 33.914391, mean_q: -50.273712\n",
      "  12200/150000: episode: 61, duration: 3.573s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 5.829415, mae: 34.117031, mean_q: -50.400711\n",
      "  12400/150000: episode: 62, duration: 3.694s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.568686, mae: 34.237858, mean_q: -50.617832\n",
      "  12600/150000: episode: 63, duration: 3.506s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.062361, mae: 34.411781, mean_q: -51.004395\n",
      "  12800/150000: episode: 64, duration: 3.615s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.955487, mae: 34.537827, mean_q: -51.206455\n",
      "  13000/150000: episode: 65, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.721275, mae: 34.765896, mean_q: -51.527130\n",
      "  13200/150000: episode: 66, duration: 3.745s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.139581, mae: 34.945473, mean_q: -51.755165\n",
      "  13400/150000: episode: 67, duration: 3.512s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.172180, mae: 35.114040, mean_q: -52.015118\n",
      "  13600/150000: episode: 68, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.973365, mae: 35.275780, mean_q: -52.251068\n",
      "  13800/150000: episode: 69, duration: 3.551s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 4.909697, mae: 35.502392, mean_q: -52.631172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14000/150000: episode: 70, duration: 3.767s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.310924, mae: 35.670403, mean_q: -52.835567\n",
      "  14200/150000: episode: 71, duration: 3.767s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.669079, mae: 35.754822, mean_q: -52.918766\n",
      "  14400/150000: episode: 72, duration: 3.841s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 4.750194, mae: 35.933838, mean_q: -53.359482\n",
      "  14600/150000: episode: 73, duration: 3.739s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.724871, mae: 36.065506, mean_q: -53.361401\n",
      "  14800/150000: episode: 74, duration: 3.921s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.783579, mae: 36.086285, mean_q: -53.296066\n",
      "  15000/150000: episode: 75, duration: 3.573s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.952435, mae: 36.151554, mean_q: -53.600029\n",
      "  15200/150000: episode: 76, duration: 3.793s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 7.365075, mae: 36.316811, mean_q: -53.789066\n",
      "  15400/150000: episode: 77, duration: 3.639s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.817384, mae: 36.437469, mean_q: -53.831249\n",
      "  15600/150000: episode: 78, duration: 3.614s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 7.449069, mae: 36.475529, mean_q: -54.016590\n",
      "  15800/150000: episode: 79, duration: 3.647s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.704231, mae: 36.526367, mean_q: -53.968056\n",
      "  16000/150000: episode: 80, duration: 3.543s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.573294, mae: 36.745461, mean_q: -54.294479\n",
      "  16200/150000: episode: 81, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 6.847855, mae: 36.807686, mean_q: -54.561718\n",
      "  16400/150000: episode: 82, duration: 3.645s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.308328, mae: 37.004379, mean_q: -54.815296\n",
      "  16600/150000: episode: 83, duration: 3.588s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.601998, mae: 37.114609, mean_q: -54.971176\n",
      "  16800/150000: episode: 84, duration: 3.679s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.930389, mae: 37.194054, mean_q: -54.929546\n",
      "  17000/150000: episode: 85, duration: 3.580s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.323574, mae: 37.150867, mean_q: -54.914692\n",
      "  17200/150000: episode: 86, duration: 3.635s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.055966, mae: 37.207729, mean_q: -55.130142\n",
      "  17400/150000: episode: 87, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 7.762090, mae: 37.282909, mean_q: -55.257442\n",
      "  17600/150000: episode: 88, duration: 3.453s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.775706, mae: 37.496262, mean_q: -55.578907\n",
      "  17800/150000: episode: 89, duration: 3.706s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.417347, mae: 37.573586, mean_q: -55.552837\n",
      "  18000/150000: episode: 90, duration: 3.707s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.665963, mae: 37.751873, mean_q: -55.903526\n",
      "  18200/150000: episode: 91, duration: 3.738s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.310789, mae: 37.818459, mean_q: -56.129128\n",
      "  18400/150000: episode: 92, duration: 3.674s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.801930, mae: 38.005558, mean_q: -56.230526\n",
      "  18600/150000: episode: 93, duration: 3.589s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.708459, mae: 38.200558, mean_q: -56.557320\n",
      "  18800/150000: episode: 94, duration: 3.676s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 12.546745, mae: 38.173729, mean_q: -56.209072\n",
      "  19000/150000: episode: 95, duration: 3.623s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.839784, mae: 38.128860, mean_q: -56.521854\n",
      "  19200/150000: episode: 96, duration: 3.740s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.107065, mae: 38.235153, mean_q: -56.791359\n",
      "  19400/150000: episode: 97, duration: 3.694s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.532758, mae: 38.372574, mean_q: -56.941299\n",
      "  19600/150000: episode: 98, duration: 3.582s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.246761, mae: 38.466534, mean_q: -56.929127\n",
      "  19800/150000: episode: 99, duration: 3.646s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.797506, mae: 38.579102, mean_q: -57.276730\n",
      "  20000/150000: episode: 100, duration: 3.713s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.479578, mae: 38.831646, mean_q: -57.699806\n",
      "  20200/150000: episode: 101, duration: 3.735s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.372718, mae: 38.980366, mean_q: -57.757156\n",
      "  20400/150000: episode: 102, duration: 3.713s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.473489, mae: 39.055450, mean_q: -57.737949\n",
      "  20600/150000: episode: 103, duration: 3.682s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.566690, mae: 38.910328, mean_q: -57.570606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20800/150000: episode: 104, duration: 3.832s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 9.429422, mae: 38.938541, mean_q: -57.562714\n",
      "  21000/150000: episode: 105, duration: 3.696s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.870944, mae: 38.836235, mean_q: -57.392773\n",
      "  21200/150000: episode: 106, duration: 3.692s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 6.743615, mae: 38.917789, mean_q: -57.728184\n",
      "  21400/150000: episode: 107, duration: 3.670s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 8.313572, mae: 39.051708, mean_q: -57.781178\n",
      "  21600/150000: episode: 108, duration: 3.907s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.114810, mae: 39.155861, mean_q: -58.038658\n",
      "  21800/150000: episode: 109, duration: 4.009s, episode steps: 200, steps per second:  50, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.140106, mae: 39.230515, mean_q: -58.236378\n",
      "  22000/150000: episode: 110, duration: 3.932s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.974501, mae: 39.411583, mean_q: -58.445572\n",
      "  22200/150000: episode: 111, duration: 3.882s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.589616, mae: 39.550270, mean_q: -58.430313\n",
      "  22400/150000: episode: 112, duration: 3.878s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.812918, mae: 39.435284, mean_q: -58.532372\n",
      "  22600/150000: episode: 113, duration: 3.950s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 11.048212, mae: 39.431995, mean_q: -58.256386\n",
      "  22800/150000: episode: 114, duration: 3.984s, episode steps: 200, steps per second:  50, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 6.871343, mae: 39.444523, mean_q: -58.557053\n",
      "  23000/150000: episode: 115, duration: 3.882s, episode steps: 200, steps per second:  52, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.943913, mae: 39.513981, mean_q: -58.545834\n",
      "  23200/150000: episode: 116, duration: 3.904s, episode steps: 200, steps per second:  51, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.761053, mae: 39.477306, mean_q: -58.442139\n",
      "  23400/150000: episode: 117, duration: 3.735s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.391944, mae: 39.566586, mean_q: -58.594063\n",
      "  23600/150000: episode: 118, duration: 3.711s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.501820, mae: 39.596817, mean_q: -58.694637\n",
      "  23800/150000: episode: 119, duration: 3.652s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.648961, mae: 39.677017, mean_q: -58.856827\n",
      "  24000/150000: episode: 120, duration: 3.557s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.378198, mae: 39.726910, mean_q: -58.836636\n",
      "  24200/150000: episode: 121, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.055542, mae: 39.768337, mean_q: -58.883144\n",
      "  24400/150000: episode: 122, duration: 3.700s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 9.619483, mae: 39.738560, mean_q: -58.857521\n",
      "  24600/150000: episode: 123, duration: 3.674s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 6.706066, mae: 39.837204, mean_q: -59.127132\n",
      "  24800/150000: episode: 124, duration: 3.493s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.607330, mae: 39.973919, mean_q: -59.346439\n",
      "  25000/150000: episode: 125, duration: 3.628s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.574022, mae: 40.145962, mean_q: -59.567604\n",
      "  25200/150000: episode: 126, duration: 3.696s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.964753, mae: 40.166996, mean_q: -59.531807\n",
      "  25400/150000: episode: 127, duration: 3.579s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 8.080540, mae: 40.127735, mean_q: -59.517120\n",
      "  25600/150000: episode: 128, duration: 3.605s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.579220, mae: 40.164532, mean_q: -59.440517\n",
      "  25800/150000: episode: 129, duration: 3.735s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.138530, mae: 40.122482, mean_q: -59.461082\n",
      "  26000/150000: episode: 130, duration: 3.674s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.906955, mae: 40.165024, mean_q: -59.386509\n",
      "  26200/150000: episode: 131, duration: 3.729s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 10.557080, mae: 40.186005, mean_q: -59.488258\n",
      "  26400/150000: episode: 132, duration: 3.547s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.331240, mae: 40.155880, mean_q: -59.554100\n",
      "  26600/150000: episode: 133, duration: 3.707s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.416734, mae: 40.180435, mean_q: -59.420704\n",
      "  26800/150000: episode: 134, duration: 3.787s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.768339, mae: 40.269382, mean_q: -59.590092\n",
      "  27000/150000: episode: 135, duration: 3.653s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.301209, mae: 40.193111, mean_q: -59.493153\n",
      "  27200/150000: episode: 136, duration: 3.562s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.984716, mae: 40.069702, mean_q: -59.318867\n",
      "  27400/150000: episode: 137, duration: 3.665s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.342011, mae: 40.046494, mean_q: -59.274582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  27600/150000: episode: 138, duration: 3.809s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 8.827886, mae: 40.112144, mean_q: -59.377430\n",
      "  27800/150000: episode: 139, duration: 3.744s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.151934, mae: 40.132145, mean_q: -59.428417\n",
      "  28000/150000: episode: 140, duration: 3.575s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.119085, mae: 40.153446, mean_q: -59.406406\n",
      "  28200/150000: episode: 141, duration: 3.535s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.520949, mae: 40.084965, mean_q: -59.339218\n",
      "  28400/150000: episode: 142, duration: 3.702s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.593525, mae: 40.058472, mean_q: -59.409882\n",
      "  28600/150000: episode: 143, duration: 3.739s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.891614, mae: 40.195648, mean_q: -59.714161\n",
      "  28800/150000: episode: 144, duration: 3.652s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.512738, mae: 40.240459, mean_q: -59.589043\n",
      "  29000/150000: episode: 145, duration: 3.709s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.994452, mae: 40.315746, mean_q: -59.768456\n",
      "  29200/150000: episode: 146, duration: 3.736s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.904625, mae: 40.373863, mean_q: -59.769455\n",
      "  29400/150000: episode: 147, duration: 3.782s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.808402, mae: 40.344982, mean_q: -59.846840\n",
      "  29600/150000: episode: 148, duration: 3.718s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.570217, mae: 40.506191, mean_q: -60.199429\n",
      "  29800/150000: episode: 149, duration: 3.743s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 10.190059, mae: 40.596554, mean_q: -60.061855\n",
      "  30000/150000: episode: 150, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 10.884526, mae: 40.449242, mean_q: -59.815128\n",
      "  30200/150000: episode: 151, duration: 3.607s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 11.448507, mae: 40.418556, mean_q: -59.801682\n",
      "  30400/150000: episode: 152, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 5.451939, mae: 40.416870, mean_q: -60.054062\n",
      "  30600/150000: episode: 153, duration: 3.723s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.681653, mae: 40.423042, mean_q: -59.880878\n",
      "  30800/150000: episode: 154, duration: 3.706s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.678110, mae: 40.532486, mean_q: -60.043320\n",
      "  31000/150000: episode: 155, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.182345, mae: 40.554512, mean_q: -60.081009\n",
      "  31200/150000: episode: 156, duration: 3.640s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.218742, mae: 40.617626, mean_q: -60.106659\n",
      "  31400/150000: episode: 157, duration: 3.754s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.383348, mae: 40.626671, mean_q: -60.125050\n",
      "  31600/150000: episode: 158, duration: 3.733s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.566214, mae: 40.707462, mean_q: -60.377827\n",
      "  31800/150000: episode: 159, duration: 3.555s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.132148, mae: 40.747528, mean_q: -60.398350\n",
      "  32000/150000: episode: 160, duration: 3.726s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.232934, mae: 40.895378, mean_q: -60.654953\n",
      "  32200/150000: episode: 161, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.610157, mae: 41.073105, mean_q: -60.933609\n",
      "  32400/150000: episode: 162, duration: 3.627s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 11.709194, mae: 41.041660, mean_q: -60.602959\n",
      "  32600/150000: episode: 163, duration: 3.597s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 11.429688, mae: 40.930550, mean_q: -60.528107\n",
      "  32800/150000: episode: 164, duration: 3.683s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.570112, mae: 40.827480, mean_q: -60.526836\n",
      "  33000/150000: episode: 165, duration: 3.529s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 9.338976, mae: 40.866940, mean_q: -60.540401\n",
      "  33200/150000: episode: 166, duration: 3.331s, episode steps: 200, steps per second:  60, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.906801, mae: 40.985973, mean_q: -60.812111\n",
      "  33400/150000: episode: 167, duration: 3.594s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.660867, mae: 41.024796, mean_q: -60.756744\n",
      "  33600/150000: episode: 168, duration: 3.683s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.340956, mae: 41.047104, mean_q: -60.770264\n",
      "  33800/150000: episode: 169, duration: 3.618s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.248476, mae: 41.063210, mean_q: -60.901749\n",
      "  34000/150000: episode: 170, duration: 3.572s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.095779, mae: 41.097851, mean_q: -60.977074\n",
      "  34200/150000: episode: 171, duration: 3.616s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.663656, mae: 41.226505, mean_q: -61.077686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34400/150000: episode: 172, duration: 3.561s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.667616, mae: 41.075577, mean_q: -60.929478\n",
      "  34600/150000: episode: 173, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.393384, mae: 41.140156, mean_q: -61.026642\n",
      "  34800/150000: episode: 174, duration: 3.685s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.657503, mae: 41.132442, mean_q: -60.939453\n",
      "  35000/150000: episode: 175, duration: 3.588s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.528314, mae: 41.016964, mean_q: -60.756104\n",
      "  35200/150000: episode: 176, duration: 3.466s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.642421, mae: 40.878582, mean_q: -60.559986\n",
      "  35400/150000: episode: 177, duration: 3.614s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 10.698081, mae: 40.805126, mean_q: -60.370209\n",
      "  35600/150000: episode: 178, duration: 3.610s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.402406, mae: 40.764011, mean_q: -60.381714\n",
      "  35800/150000: episode: 179, duration: 3.549s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.702303, mae: 40.841965, mean_q: -60.544044\n",
      "  36000/150000: episode: 180, duration: 3.649s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 10.702036, mae: 40.856243, mean_q: -60.497665\n",
      "  36200/150000: episode: 181, duration: 3.699s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.684834, mae: 40.863480, mean_q: -60.637100\n",
      "  36400/150000: episode: 182, duration: 3.435s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.869991, mae: 40.898510, mean_q: -60.633354\n",
      "  36600/150000: episode: 183, duration: 3.603s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 10.877520, mae: 40.863491, mean_q: -60.459648\n",
      "  36800/150000: episode: 184, duration: 3.574s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.385354, mae: 40.753395, mean_q: -60.354828\n",
      "  37000/150000: episode: 185, duration: 3.585s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.643404, mae: 40.738022, mean_q: -60.299820\n",
      "  37200/150000: episode: 186, duration: 3.692s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 6.008712, mae: 40.904690, mean_q: -60.739361\n",
      "  37400/150000: episode: 187, duration: 3.475s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.863934, mae: 40.959206, mean_q: -60.723827\n",
      "  37600/150000: episode: 188, duration: 3.609s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.652924, mae: 40.993309, mean_q: -60.699776\n",
      "  37800/150000: episode: 189, duration: 3.577s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 5.990509, mae: 40.999058, mean_q: -60.868584\n",
      "  38000/150000: episode: 190, duration: 3.590s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.550066, mae: 41.109112, mean_q: -60.926964\n",
      "  38200/150000: episode: 191, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 5.873422, mae: 41.191841, mean_q: -61.167431\n",
      "  38400/150000: episode: 192, duration: 3.584s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 8.786522, mae: 41.292397, mean_q: -61.142784\n",
      "  38600/150000: episode: 193, duration: 3.686s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.281595, mae: 41.303589, mean_q: -61.237667\n",
      "  38800/150000: episode: 194, duration: 3.690s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 11.839991, mae: 41.295753, mean_q: -61.154591\n",
      "  39000/150000: episode: 195, duration: 3.725s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.749764, mae: 41.320732, mean_q: -61.441185\n",
      "  39200/150000: episode: 196, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.248989, mae: 41.415039, mean_q: -61.451176\n",
      "  39400/150000: episode: 197, duration: 3.562s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.929053, mae: 41.389648, mean_q: -61.347279\n",
      "  39600/150000: episode: 198, duration: 3.463s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 9.927896, mae: 41.320663, mean_q: -61.220657\n",
      "  39800/150000: episode: 199, duration: 3.539s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.445354, mae: 41.256538, mean_q: -61.139549\n",
      "  40000/150000: episode: 200, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.476471, mae: 41.328671, mean_q: -61.322803\n",
      "  40200/150000: episode: 201, duration: 3.515s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.905332, mae: 41.388115, mean_q: -61.377731\n",
      "  40400/150000: episode: 202, duration: 3.717s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 10.539001, mae: 41.475712, mean_q: -61.409122\n",
      "  40600/150000: episode: 203, duration: 3.637s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.112947, mae: 41.435307, mean_q: -61.468121\n",
      "  40800/150000: episode: 204, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 9.295176, mae: 41.419044, mean_q: -61.369335\n",
      "  41000/150000: episode: 205, duration: 3.709s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.771467, mae: 41.479458, mean_q: -61.471718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41200/150000: episode: 206, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 9.122383, mae: 41.542133, mean_q: -61.542789\n",
      "  41400/150000: episode: 207, duration: 3.618s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 9.666497, mae: 41.508526, mean_q: -61.490704\n",
      "  41600/150000: episode: 208, duration: 3.687s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 10.179127, mae: 41.549442, mean_q: -61.506836\n",
      "  41800/150000: episode: 209, duration: 3.629s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.018546, mae: 41.422752, mean_q: -61.327255\n",
      "  42000/150000: episode: 210, duration: 3.527s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.479342, mae: 41.363781, mean_q: -61.321701\n",
      "  42200/150000: episode: 211, duration: 3.566s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.300899, mae: 41.469154, mean_q: -61.610401\n",
      "  42400/150000: episode: 212, duration: 3.564s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 6.907100, mae: 41.562885, mean_q: -61.706955\n",
      "  42600/150000: episode: 213, duration: 3.650s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.189012, mae: 41.686432, mean_q: -61.865292\n",
      "  42800/150000: episode: 214, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.962618, mae: 41.588707, mean_q: -61.588879\n",
      "  43000/150000: episode: 215, duration: 3.535s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 11.032870, mae: 41.563419, mean_q: -61.534111\n",
      "  43200/150000: episode: 216, duration: 3.613s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.650163, mae: 41.630516, mean_q: -61.716728\n",
      "  43400/150000: episode: 217, duration: 3.680s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 10.610427, mae: 41.566666, mean_q: -61.573524\n",
      "  43600/150000: episode: 218, duration: 3.563s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 10.487241, mae: 41.397221, mean_q: -61.272236\n",
      "  43800/150000: episode: 219, duration: 3.707s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 10.552662, mae: 41.416748, mean_q: -61.220337\n",
      "  44000/150000: episode: 220, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.077142, mae: 41.357853, mean_q: -61.443554\n",
      "  44200/150000: episode: 221, duration: 3.641s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.883028, mae: 41.430252, mean_q: -61.422901\n",
      "  44400/150000: episode: 222, duration: 3.556s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.543022, mae: 41.444260, mean_q: -61.361282\n",
      "  44600/150000: episode: 223, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.163964, mae: 41.415558, mean_q: -61.485073\n",
      "  44800/150000: episode: 224, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.610219, mae: 41.469627, mean_q: -61.428635\n",
      "  45000/150000: episode: 225, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.717024, mae: 41.487774, mean_q: -61.526745\n",
      "  45200/150000: episode: 226, duration: 3.560s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 10.072826, mae: 41.532921, mean_q: -61.511585\n",
      "  45400/150000: episode: 227, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.919326, mae: 41.292778, mean_q: -60.974308\n",
      "  45600/150000: episode: 228, duration: 3.547s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 12.102448, mae: 41.167248, mean_q: -60.908974\n",
      "  45800/150000: episode: 229, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.513307, mae: 41.090038, mean_q: -60.829521\n",
      "  46000/150000: episode: 230, duration: 3.686s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.245236, mae: 41.197578, mean_q: -61.157295\n",
      "  46200/150000: episode: 231, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 14.433676, mae: 40.950199, mean_q: -60.413242\n",
      "  46400/150000: episode: 232, duration: 3.673s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.446560, mae: 40.869965, mean_q: -60.585236\n",
      "  46600/150000: episode: 233, duration: 3.628s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.483896, mae: 40.830006, mean_q: -60.285721\n",
      "  46800/150000: episode: 234, duration: 3.609s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 6.788912, mae: 40.928585, mean_q: -60.765957\n",
      "  47000/150000: episode: 235, duration: 3.663s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 10.092665, mae: 40.884350, mean_q: -60.424072\n",
      "  47200/150000: episode: 236, duration: 3.649s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 8.661276, mae: 40.712315, mean_q: -60.343658\n",
      "  47400/150000: episode: 237, duration: 3.539s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.900661, mae: 40.646629, mean_q: -60.237350\n",
      "  47600/150000: episode: 238, duration: 3.702s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.234720, mae: 40.674252, mean_q: -60.290073\n",
      "  47800/150000: episode: 239, duration: 3.701s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.897918, mae: 40.618355, mean_q: -60.091618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48000/150000: episode: 240, duration: 3.487s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 5.417083, mae: 40.687531, mean_q: -60.383202\n",
      "  48200/150000: episode: 241, duration: 3.670s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.105212, mae: 40.726429, mean_q: -60.407761\n",
      "  48400/150000: episode: 242, duration: 3.549s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.980595, mae: 40.781216, mean_q: -60.445057\n",
      "  48600/150000: episode: 243, duration: 3.801s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 7.146545, mae: 40.885082, mean_q: -60.638683\n",
      "  48800/150000: episode: 244, duration: 3.695s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.794394, mae: 40.986660, mean_q: -60.798317\n",
      "  49000/150000: episode: 245, duration: 3.570s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.449118, mae: 41.066006, mean_q: -60.982590\n",
      "  49200/150000: episode: 246, duration: 3.592s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.337543, mae: 41.063408, mean_q: -60.906551\n",
      "  49400/150000: episode: 247, duration: 3.530s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 10.806123, mae: 41.098587, mean_q: -60.726273\n",
      "  49600/150000: episode: 248, duration: 3.718s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.554221, mae: 40.987392, mean_q: -60.664970\n",
      "  49800/150000: episode: 249, duration: 3.742s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.563667, mae: 40.971424, mean_q: -60.687611\n",
      "  50000/150000: episode: 250, duration: 3.663s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.949305, mae: 40.813934, mean_q: -60.484798\n",
      "  50200/150000: episode: 251, duration: 3.663s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.340690, mae: 40.898388, mean_q: -60.651161\n",
      "  50400/150000: episode: 252, duration: 3.652s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.069605, mae: 40.952946, mean_q: -60.719639\n",
      "  50600/150000: episode: 253, duration: 3.640s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.572226, mae: 40.873138, mean_q: -60.580334\n",
      "  50800/150000: episode: 254, duration: 3.664s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 7.532856, mae: 40.847836, mean_q: -60.526260\n",
      "  51000/150000: episode: 255, duration: 3.720s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.742475, mae: 40.897190, mean_q: -60.704086\n",
      "  51200/150000: episode: 256, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.896338, mae: 40.926044, mean_q: -60.581928\n",
      "  51400/150000: episode: 257, duration: 3.626s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.276808, mae: 40.842457, mean_q: -60.472641\n",
      "  51600/150000: episode: 258, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.417266, mae: 40.777370, mean_q: -60.347557\n",
      "  51800/150000: episode: 259, duration: 3.546s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 10.149897, mae: 40.667801, mean_q: -60.223515\n",
      "  52000/150000: episode: 260, duration: 3.639s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.706562, mae: 40.681046, mean_q: -60.244705\n",
      "  52200/150000: episode: 261, duration: 3.709s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 11.512176, mae: 40.575611, mean_q: -60.071377\n",
      "  52400/150000: episode: 262, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.427641, mae: 40.605927, mean_q: -60.177670\n",
      "  52600/150000: episode: 263, duration: 3.699s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.213732, mae: 40.664013, mean_q: -60.252541\n",
      "  52800/150000: episode: 264, duration: 3.646s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 10.057326, mae: 40.648647, mean_q: -59.984440\n",
      "  53000/150000: episode: 265, duration: 3.695s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 11.824481, mae: 40.360889, mean_q: -59.674747\n",
      "  53200/150000: episode: 266, duration: 3.573s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.850414, mae: 40.448227, mean_q: -60.025978\n",
      "  53400/150000: episode: 267, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.797753, mae: 40.439938, mean_q: -59.915585\n",
      "  53600/150000: episode: 268, duration: 3.524s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.791123, mae: 40.415531, mean_q: -59.782108\n",
      "  53800/150000: episode: 269, duration: 3.609s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 11.264085, mae: 40.094349, mean_q: -59.274635\n",
      "  54000/150000: episode: 270, duration: 3.638s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.422148, mae: 40.030018, mean_q: -59.219975\n",
      "  54200/150000: episode: 271, duration: 3.756s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.195712, mae: 40.039494, mean_q: -59.338551\n",
      "  54400/150000: episode: 272, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 7.921011, mae: 40.015392, mean_q: -59.265747\n",
      "  54600/150000: episode: 273, duration: 3.583s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 6.779292, mae: 40.027611, mean_q: -59.403217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  54800/150000: episode: 274, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.075037, mae: 40.157242, mean_q: -59.559494\n",
      "  55000/150000: episode: 275, duration: 3.688s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 9.401329, mae: 40.087219, mean_q: -59.370331\n",
      "  55200/150000: episode: 276, duration: 3.696s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.968007, mae: 40.141178, mean_q: -59.628292\n",
      "  55400/150000: episode: 277, duration: 3.688s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.022139, mae: 40.148163, mean_q: -59.364120\n",
      "  55600/150000: episode: 278, duration: 3.661s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 10.144995, mae: 40.081348, mean_q: -59.178341\n",
      "  55800/150000: episode: 279, duration: 3.570s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.803572, mae: 39.870033, mean_q: -59.096737\n",
      "  56000/150000: episode: 280, duration: 3.576s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 8.748111, mae: 39.714741, mean_q: -58.821098\n",
      "  56200/150000: episode: 281, duration: 3.670s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.378180, mae: 39.798481, mean_q: -58.957516\n",
      "  56400/150000: episode: 282, duration: 3.513s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.516862, mae: 39.750080, mean_q: -58.791359\n",
      "  56600/150000: episode: 283, duration: 3.592s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 9.323854, mae: 39.730347, mean_q: -58.824875\n",
      "  56800/150000: episode: 284, duration: 3.673s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 8.057223, mae: 39.783264, mean_q: -58.953087\n",
      "  57000/150000: episode: 285, duration: 3.579s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.000628, mae: 39.649342, mean_q: -58.695107\n",
      "  57200/150000: episode: 286, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.286066, mae: 39.608021, mean_q: -58.713497\n",
      "  57400/150000: episode: 287, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 10.982391, mae: 39.455597, mean_q: -58.211914\n",
      "  57600/150000: episode: 288, duration: 3.422s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.523760, mae: 39.404316, mean_q: -58.373360\n",
      "  57800/150000: episode: 289, duration: 3.550s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 7.395708, mae: 39.410580, mean_q: -58.426464\n",
      "  58000/150000: episode: 290, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.346033, mae: 39.322174, mean_q: -58.182720\n",
      "  58200/150000: episode: 291, duration: 3.587s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 6.707012, mae: 39.252926, mean_q: -58.133526\n",
      "  58400/150000: episode: 292, duration: 3.576s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 5.889716, mae: 39.297699, mean_q: -58.304855\n",
      "  58600/150000: episode: 293, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.513506, mae: 39.379421, mean_q: -58.332035\n",
      "  58800/150000: episode: 294, duration: 3.555s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.279697, mae: 39.199352, mean_q: -57.985859\n",
      "  59000/150000: episode: 295, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.273656, mae: 39.088615, mean_q: -57.803169\n",
      "  59200/150000: episode: 296, duration: 3.608s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 9.161495, mae: 38.991676, mean_q: -57.622746\n",
      "  59400/150000: episode: 297, duration: 3.565s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 9.552011, mae: 38.858814, mean_q: -57.512394\n",
      "  59600/150000: episode: 298, duration: 3.563s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.569247, mae: 38.960960, mean_q: -57.790359\n",
      "  59800/150000: episode: 299, duration: 3.679s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.528918, mae: 39.069321, mean_q: -57.898643\n",
      "  60000/150000: episode: 300, duration: 3.525s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 10.273951, mae: 39.013527, mean_q: -57.565304\n",
      "  60200/150000: episode: 301, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 9.130261, mae: 38.675911, mean_q: -57.175987\n",
      "  60400/150000: episode: 302, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.651191, mae: 38.716866, mean_q: -57.360840\n",
      "  60600/150000: episode: 303, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.506143, mae: 38.780922, mean_q: -57.330929\n",
      "  60800/150000: episode: 304, duration: 3.568s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 6.090127, mae: 38.797054, mean_q: -57.488392\n",
      "  61000/150000: episode: 305, duration: 3.595s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 6.629796, mae: 38.758987, mean_q: -57.443939\n",
      "  61200/150000: episode: 306, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 12.482265, mae: 38.586246, mean_q: -57.029747\n",
      "  61400/150000: episode: 307, duration: 3.538s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 9.505545, mae: 38.507904, mean_q: -56.936893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  61600/150000: episode: 308, duration: 3.600s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.895343, mae: 38.492584, mean_q: -56.946503\n",
      "  61800/150000: episode: 309, duration: 3.610s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.472767, mae: 38.443680, mean_q: -56.967804\n",
      "  62000/150000: episode: 310, duration: 3.459s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.089652, mae: 38.461544, mean_q: -56.977783\n",
      "  62200/150000: episode: 311, duration: 3.629s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 9.792767, mae: 38.539135, mean_q: -57.039532\n",
      "  62400/150000: episode: 312, duration: 3.685s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 8.689963, mae: 38.518497, mean_q: -56.999844\n",
      "  62600/150000: episode: 313, duration: 3.613s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.698629, mae: 38.518963, mean_q: -57.049461\n",
      "  62800/150000: episode: 314, duration: 3.716s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 8.142675, mae: 38.324581, mean_q: -56.735912\n",
      "  63000/150000: episode: 315, duration: 3.646s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 6.610733, mae: 38.470852, mean_q: -57.009499\n",
      "  63200/150000: episode: 316, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 11.100220, mae: 38.409237, mean_q: -56.699337\n",
      "  63400/150000: episode: 317, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 8.357813, mae: 38.275867, mean_q: -56.557770\n",
      "  63600/150000: episode: 318, duration: 3.605s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 11.540234, mae: 38.141430, mean_q: -56.318748\n",
      "  63800/150000: episode: 319, duration: 3.533s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 7.881740, mae: 38.118412, mean_q: -56.380001\n",
      "  64000/150000: episode: 320, duration: 3.591s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.461312, mae: 37.963726, mean_q: -56.140713\n",
      "  64200/150000: episode: 321, duration: 3.610s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 8.695835, mae: 37.957794, mean_q: -56.184380\n",
      "  64400/150000: episode: 322, duration: 3.628s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.695 [0.000, 2.000],  loss: 8.844995, mae: 37.791763, mean_q: -55.728172\n",
      "  64600/150000: episode: 323, duration: 3.519s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.500202, mae: 37.716957, mean_q: -55.708496\n",
      "  64800/150000: episode: 324, duration: 3.607s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.980999, mae: 37.568127, mean_q: -55.580097\n",
      "  65000/150000: episode: 325, duration: 3.595s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 5.968390, mae: 37.592392, mean_q: -55.683441\n",
      "  65200/150000: episode: 326, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 7.362229, mae: 37.282745, mean_q: -55.052006\n",
      "  65400/150000: episode: 327, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 7.628041, mae: 37.301437, mean_q: -54.997997\n",
      "  65600/150000: episode: 328, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.625 [0.000, 2.000],  loss: 7.363927, mae: 36.915562, mean_q: -54.496395\n",
      "  65800/150000: episode: 329, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 9.982960, mae: 36.729794, mean_q: -54.167240\n",
      "  66000/150000: episode: 330, duration: 3.706s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 7.672531, mae: 36.607639, mean_q: -54.062641\n",
      "  66200/150000: episode: 331, duration: 3.533s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.572280, mae: 36.558884, mean_q: -54.039120\n",
      "  66400/150000: episode: 332, duration: 3.691s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.655 [0.000, 2.000],  loss: 6.542509, mae: 36.390797, mean_q: -53.683693\n",
      "  66600/150000: episode: 333, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 7.611373, mae: 36.177563, mean_q: -53.439396\n",
      "  66800/150000: episode: 334, duration: 3.538s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 7.584401, mae: 36.059048, mean_q: -53.215031\n",
      "  67000/150000: episode: 335, duration: 3.674s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 7.749377, mae: 35.859909, mean_q: -52.906883\n",
      "  67200/150000: episode: 336, duration: 3.618s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.792419, mae: 35.807163, mean_q: -52.865459\n",
      "  67400/150000: episode: 337, duration: 3.673s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.650 [0.000, 2.000],  loss: 4.675394, mae: 35.734039, mean_q: -52.837147\n",
      "  67600/150000: episode: 338, duration: 3.668s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000],  loss: 9.477209, mae: 35.591579, mean_q: -52.432793\n",
      "  67800/150000: episode: 339, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 7.012285, mae: 35.401318, mean_q: -52.186417\n",
      "  68000/150000: episode: 340, duration: 3.653s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.740 [0.000, 2.000],  loss: 4.868155, mae: 35.342438, mean_q: -52.256752\n",
      "  68200/150000: episode: 341, duration: 3.602s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000],  loss: 6.745988, mae: 35.361435, mean_q: -52.220089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  68400/150000: episode: 342, duration: 3.586s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000],  loss: 6.219409, mae: 35.226974, mean_q: -52.038662\n",
      "  68600/150000: episode: 343, duration: 3.695s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 6.670076, mae: 35.244228, mean_q: -52.050518\n",
      "  68800/150000: episode: 344, duration: 3.744s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.740 [0.000, 2.000],  loss: 6.055716, mae: 35.322289, mean_q: -52.171432\n",
      "  69000/150000: episode: 345, duration: 3.547s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 5.208149, mae: 35.326851, mean_q: -52.177399\n",
      "  69200/150000: episode: 346, duration: 3.692s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000],  loss: 6.483904, mae: 35.181789, mean_q: -51.799091\n",
      "  69400/150000: episode: 347, duration: 3.598s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 5.296480, mae: 35.109028, mean_q: -51.841606\n",
      "  69600/150000: episode: 348, duration: 3.628s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 5.777311, mae: 35.128490, mean_q: -51.953644\n",
      "  69800/150000: episode: 349, duration: 3.574s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 5.107686, mae: 35.211708, mean_q: -52.087646\n",
      "  70000/150000: episode: 350, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.912822, mae: 35.086624, mean_q: -51.800575\n",
      "  70200/150000: episode: 351, duration: 3.577s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000],  loss: 5.858147, mae: 35.126884, mean_q: -51.855221\n",
      "  70400/150000: episode: 352, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 5.244378, mae: 35.168934, mean_q: -51.986534\n",
      "  70600/150000: episode: 353, duration: 3.599s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 7.738399, mae: 35.030949, mean_q: -51.638866\n",
      "  70800/150000: episode: 354, duration: 3.687s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 5.967845, mae: 35.102890, mean_q: -51.876389\n",
      "  71000/150000: episode: 355, duration: 3.615s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 6.435052, mae: 35.162476, mean_q: -52.008007\n",
      "  71200/150000: episode: 356, duration: 3.698s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.079686, mae: 35.179245, mean_q: -52.094505\n",
      "  71400/150000: episode: 357, duration: 3.728s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.299096, mae: 35.415302, mean_q: -52.380829\n",
      "  71600/150000: episode: 358, duration: 3.579s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.068348, mae: 35.438290, mean_q: -52.426510\n",
      "  71800/150000: episode: 359, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.901027, mae: 35.535206, mean_q: -52.526554\n",
      "  72000/150000: episode: 360, duration: 3.697s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.778728, mae: 35.519089, mean_q: -52.454605\n",
      "  72200/150000: episode: 361, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.316115, mae: 35.429249, mean_q: -52.241974\n",
      "  72400/150000: episode: 362, duration: 3.626s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 6.181429, mae: 35.446049, mean_q: -52.533428\n",
      "  72600/150000: episode: 363, duration: 3.716s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.813182, mae: 35.475025, mean_q: -52.467144\n",
      "  72800/150000: episode: 364, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.146483, mae: 35.464352, mean_q: -52.540604\n",
      "  73000/150000: episode: 365, duration: 3.615s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 9.064932, mae: 35.277145, mean_q: -52.038918\n",
      "  73200/150000: episode: 366, duration: 3.542s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.979150, mae: 35.176929, mean_q: -52.181660\n",
      "  73400/150000: episode: 367, duration: 3.482s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.304379, mae: 35.240810, mean_q: -52.240849\n",
      "  73600/150000: episode: 368, duration: 3.513s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.167863, mae: 35.159561, mean_q: -52.009842\n",
      "  73800/150000: episode: 369, duration: 3.537s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 7.585478, mae: 35.061302, mean_q: -51.732574\n",
      "  74000/150000: episode: 370, duration: 3.600s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 7.503320, mae: 34.820099, mean_q: -51.456284\n",
      "  74200/150000: episode: 371, duration: 3.639s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 7.350344, mae: 34.719601, mean_q: -51.296280\n",
      "  74400/150000: episode: 372, duration: 3.577s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 4.306182, mae: 34.567230, mean_q: -51.176666\n",
      "  74600/150000: episode: 373, duration: 3.641s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 6.026258, mae: 34.444443, mean_q: -50.852573\n",
      "  74800/150000: episode: 374, duration: 3.654s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 6.064919, mae: 34.267063, mean_q: -50.645908\n",
      "  75000/150000: episode: 375, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 5.985205, mae: 34.168724, mean_q: -50.566734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75200/150000: episode: 376, duration: 3.459s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.276225, mae: 34.084518, mean_q: -50.393505\n",
      "  75400/150000: episode: 377, duration: 3.700s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 5.842795, mae: 33.992886, mean_q: -50.369301\n",
      "  75600/150000: episode: 378, duration: 3.673s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 5.545202, mae: 34.100479, mean_q: -50.459663\n",
      "  75800/150000: episode: 379, duration: 3.556s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 5.715410, mae: 34.078579, mean_q: -50.394554\n",
      "  76000/150000: episode: 380, duration: 3.696s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.976814, mae: 34.059170, mean_q: -50.412724\n",
      "  76200/150000: episode: 381, duration: 3.660s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 4.486067, mae: 34.111622, mean_q: -50.580723\n",
      "  76400/150000: episode: 382, duration: 3.647s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 6.466486, mae: 34.261822, mean_q: -50.570518\n",
      "  76600/150000: episode: 383, duration: 3.655s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.069352, mae: 34.238670, mean_q: -50.651730\n",
      "  76800/150000: episode: 384, duration: 3.506s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 5.919976, mae: 34.210167, mean_q: -50.646584\n",
      "  77000/150000: episode: 385, duration: 3.567s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.762778, mae: 34.193245, mean_q: -50.473492\n",
      "  77200/150000: episode: 386, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 5.651117, mae: 34.256924, mean_q: -50.716846\n",
      "  77400/150000: episode: 387, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.716117, mae: 34.385899, mean_q: -51.015587\n",
      "  77600/150000: episode: 388, duration: 3.578s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.680660, mae: 34.483711, mean_q: -51.025185\n",
      "  77800/150000: episode: 389, duration: 3.481s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 6.629274, mae: 34.590328, mean_q: -51.195141\n",
      "  78000/150000: episode: 390, duration: 3.508s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.197104, mae: 34.711220, mean_q: -51.416222\n",
      "  78200/150000: episode: 391, duration: 3.562s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.525495, mae: 34.805965, mean_q: -51.592041\n",
      "  78400/150000: episode: 392, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 5.781536, mae: 34.923492, mean_q: -51.722149\n",
      "  78600/150000: episode: 393, duration: 3.695s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.141129, mae: 34.997543, mean_q: -51.790115\n",
      "  78800/150000: episode: 394, duration: 3.643s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.759659, mae: 35.063717, mean_q: -51.927628\n",
      "  79000/150000: episode: 395, duration: 3.643s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.464592, mae: 35.125469, mean_q: -51.923378\n",
      "  79200/150000: episode: 396, duration: 3.579s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 4.469054, mae: 35.189270, mean_q: -52.179367\n",
      "  79400/150000: episode: 397, duration: 3.578s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 5.289020, mae: 35.223797, mean_q: -52.110695\n",
      "  79600/150000: episode: 398, duration: 3.637s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 5.485963, mae: 35.157265, mean_q: -51.969833\n",
      "  79800/150000: episode: 399, duration: 3.610s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 7.016178, mae: 35.063805, mean_q: -51.712982\n",
      "  80000/150000: episode: 400, duration: 3.609s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.690 [0.000, 2.000],  loss: 5.375945, mae: 34.864822, mean_q: -51.472080\n",
      "  80200/150000: episode: 401, duration: 3.726s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 5.994956, mae: 34.665268, mean_q: -51.124092\n",
      "  80400/150000: episode: 402, duration: 3.531s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 6.625222, mae: 34.451736, mean_q: -50.811928\n",
      "  80600/150000: episode: 403, duration: 3.613s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.859215, mae: 34.287827, mean_q: -50.687729\n",
      "  80800/150000: episode: 404, duration: 3.458s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000],  loss: 4.527812, mae: 34.125526, mean_q: -50.393482\n",
      "  81000/150000: episode: 405, duration: 3.577s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 4.638111, mae: 34.094490, mean_q: -50.443729\n",
      "  81200/150000: episode: 406, duration: 3.681s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 6.928203, mae: 34.086380, mean_q: -50.212704\n",
      "  81400/150000: episode: 407, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.620 [0.000, 2.000],  loss: 6.273202, mae: 33.869267, mean_q: -49.908260\n",
      "  81600/150000: episode: 408, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 3.747691, mae: 33.792248, mean_q: -50.019276\n",
      "  81800/150000: episode: 409, duration: 3.641s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000],  loss: 8.027101, mae: 33.680859, mean_q: -49.535744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  82000/150000: episode: 410, duration: 3.607s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 6.241406, mae: 33.334408, mean_q: -49.200577\n",
      "  82200/150000: episode: 411, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 4.373890, mae: 33.469006, mean_q: -49.530399\n",
      "  82400/150000: episode: 412, duration: 3.525s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 4.200066, mae: 33.496830, mean_q: -49.534924\n",
      "  82600/150000: episode: 413, duration: 3.578s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 3.851209, mae: 33.540787, mean_q: -49.631336\n",
      "  82800/150000: episode: 414, duration: 3.605s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.553640, mae: 33.756542, mean_q: -49.872414\n",
      "  83000/150000: episode: 415, duration: 3.668s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000],  loss: 5.347554, mae: 33.763832, mean_q: -50.005234\n",
      "  83200/150000: episode: 416, duration: 3.653s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 5.376404, mae: 33.961430, mean_q: -50.306152\n",
      "  83400/150000: episode: 417, duration: 3.647s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.988277, mae: 33.852150, mean_q: -50.011524\n",
      "  83600/150000: episode: 418, duration: 3.567s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 5.542773, mae: 33.985012, mean_q: -50.275616\n",
      "  83800/150000: episode: 419, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 5.614401, mae: 34.109135, mean_q: -50.560963\n",
      "  84000/150000: episode: 420, duration: 3.713s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.986862, mae: 34.251377, mean_q: -50.482208\n",
      "  84200/150000: episode: 421, duration: 3.601s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 5.014224, mae: 34.333729, mean_q: -50.900597\n",
      "  84400/150000: episode: 422, duration: 3.468s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.713675, mae: 34.667007, mean_q: -51.364815\n",
      "  84600/150000: episode: 423, duration: 3.568s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 4.692771, mae: 34.731636, mean_q: -51.455292\n",
      "  84800/150000: episode: 424, duration: 3.676s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 6.430997, mae: 34.737610, mean_q: -51.395538\n",
      "  85000/150000: episode: 425, duration: 3.674s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 5.531017, mae: 34.888454, mean_q: -51.707016\n",
      "  85200/150000: episode: 426, duration: 3.527s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 5.817997, mae: 34.980640, mean_q: -51.749340\n",
      "  85400/150000: episode: 427, duration: 3.655s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 6.042511, mae: 34.969952, mean_q: -51.767899\n",
      "  85600/150000: episode: 428, duration: 3.751s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.075276, mae: 35.134224, mean_q: -51.865761\n",
      "  85800/150000: episode: 429, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.561018, mae: 35.040508, mean_q: -51.802334\n",
      "  86000/150000: episode: 430, duration: 3.494s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.831236, mae: 35.122562, mean_q: -51.874199\n",
      "  86200/150000: episode: 431, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 5.453788, mae: 35.276043, mean_q: -52.260067\n",
      "  86400/150000: episode: 432, duration: 3.534s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.851181, mae: 35.387882, mean_q: -52.466026\n",
      "  86600/150000: episode: 433, duration: 3.694s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 5.062374, mae: 35.630634, mean_q: -52.825787\n",
      "  86800/150000: episode: 434, duration: 3.702s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.909624, mae: 35.559380, mean_q: -52.401554\n",
      "  87000/150000: episode: 435, duration: 3.607s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.716578, mae: 35.545620, mean_q: -52.667980\n",
      "  87200/150000: episode: 436, duration: 3.510s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 7.066703, mae: 35.651348, mean_q: -52.679634\n",
      "  87400/150000: episode: 437, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.117909, mae: 35.600357, mean_q: -52.519852\n",
      "  87600/150000: episode: 438, duration: 3.491s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.062744, mae: 35.645992, mean_q: -52.793106\n",
      "  87800/150000: episode: 439, duration: 3.523s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 5.191783, mae: 35.711113, mean_q: -52.879742\n",
      "  88000/150000: episode: 440, duration: 3.526s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.570544, mae: 35.803314, mean_q: -52.859142\n",
      "  88200/150000: episode: 441, duration: 3.575s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 5.922690, mae: 35.746902, mean_q: -52.860313\n",
      "  88400/150000: episode: 442, duration: 3.638s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 7.839154, mae: 35.819504, mean_q: -52.905304\n",
      "  88600/150000: episode: 443, duration: 3.557s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 5.209722, mae: 35.901310, mean_q: -53.189911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88800/150000: episode: 444, duration: 3.685s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.990866, mae: 35.978809, mean_q: -53.167675\n",
      "  89000/150000: episode: 445, duration: 3.512s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.539508, mae: 35.889286, mean_q: -53.090069\n",
      "  89200/150000: episode: 446, duration: 3.448s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 5.162294, mae: 35.907822, mean_q: -53.121586\n",
      "  89400/150000: episode: 447, duration: 3.617s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 5.252377, mae: 35.923439, mean_q: -53.117115\n",
      "  89600/150000: episode: 448, duration: 3.682s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 6.592067, mae: 35.718910, mean_q: -52.704845\n",
      "  89800/150000: episode: 449, duration: 3.620s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.046459, mae: 35.636337, mean_q: -52.594608\n",
      "  90000/150000: episode: 450, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 5.355351, mae: 35.664703, mean_q: -52.724434\n",
      "  90200/150000: episode: 451, duration: 3.637s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000],  loss: 4.449363, mae: 35.548328, mean_q: -52.575825\n",
      "  90400/150000: episode: 452, duration: 3.584s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 4.305563, mae: 35.623844, mean_q: -52.672604\n",
      "  90600/150000: episode: 453, duration: 3.683s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 4.492507, mae: 35.454132, mean_q: -52.373856\n",
      "  90800/150000: episode: 454, duration: 3.528s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 6.258469, mae: 35.542458, mean_q: -52.463654\n",
      "  91000/150000: episode: 455, duration: 3.451s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.710 [0.000, 2.000],  loss: 7.177198, mae: 35.394665, mean_q: -52.152901\n",
      "  91200/150000: episode: 456, duration: 3.584s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.322993, mae: 35.213234, mean_q: -51.926884\n",
      "  91400/150000: episode: 457, duration: 3.597s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 5.344515, mae: 35.146133, mean_q: -51.860172\n",
      "  91600/150000: episode: 458, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 4.396039, mae: 34.964161, mean_q: -51.678200\n",
      "  91800/150000: episode: 459, duration: 3.619s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000],  loss: 6.004840, mae: 34.814209, mean_q: -51.375439\n",
      "  92000/150000: episode: 460, duration: 3.653s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.838208, mae: 34.651302, mean_q: -51.199108\n",
      "  92200/150000: episode: 461, duration: 3.688s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 3.827987, mae: 34.468048, mean_q: -50.909859\n",
      "  92400/150000: episode: 462, duration: 3.594s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 4.667387, mae: 34.552372, mean_q: -51.019863\n",
      "  92600/150000: episode: 463, duration: 3.591s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.508023, mae: 34.224903, mean_q: -50.439579\n",
      "  92800/150000: episode: 464, duration: 3.464s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.546848, mae: 34.093292, mean_q: -50.357182\n",
      "  93000/150000: episode: 465, duration: 3.679s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 6.418425, mae: 33.819374, mean_q: -49.777576\n",
      "  93200/150000: episode: 466, duration: 3.737s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.777519, mae: 33.599667, mean_q: -49.531082\n",
      "  93400/150000: episode: 467, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 5.311348, mae: 33.455837, mean_q: -49.335892\n",
      "  93600/150000: episode: 468, duration: 3.618s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.955541, mae: 33.205345, mean_q: -48.976341\n",
      "  93800/150000: episode: 469, duration: 3.562s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 4.900930, mae: 32.960033, mean_q: -48.575523\n",
      "  94000/150000: episode: 470, duration: 3.697s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 3.567774, mae: 32.967361, mean_q: -48.680283\n",
      "  94200/150000: episode: 471, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 4.125925, mae: 32.535694, mean_q: -47.951229\n",
      "  94400/150000: episode: 472, duration: 3.756s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.922503, mae: 32.317127, mean_q: -47.613827\n",
      "  94600/150000: episode: 473, duration: 3.606s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.914592, mae: 32.345360, mean_q: -47.809738\n",
      "  94800/150000: episode: 474, duration: 3.524s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.030545, mae: 32.414234, mean_q: -47.848045\n",
      "  95000/150000: episode: 475, duration: 3.594s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 4.683526, mae: 32.324791, mean_q: -47.635323\n",
      "  95200/150000: episode: 476, duration: 3.611s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.233240, mae: 32.104401, mean_q: -47.441429\n",
      "  95400/150000: episode: 477, duration: 3.693s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 4.054461, mae: 32.161625, mean_q: -47.441738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  95600/150000: episode: 478, duration: 3.587s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 3.212555, mae: 32.147484, mean_q: -47.467140\n",
      "  95800/150000: episode: 479, duration: 3.717s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 3.827524, mae: 31.973495, mean_q: -47.160168\n",
      "  96000/150000: episode: 480, duration: 3.639s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 3.786734, mae: 31.925745, mean_q: -47.079746\n",
      "  96200/150000: episode: 481, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.788225, mae: 31.758984, mean_q: -46.692593\n",
      "  96400/150000: episode: 482, duration: 3.549s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 1.965295, mae: 31.788872, mean_q: -46.954872\n",
      "  96600/150000: episode: 483, duration: 3.583s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.396238, mae: 31.762730, mean_q: -46.859608\n",
      "  96800/150000: episode: 484, duration: 3.580s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.887952, mae: 31.696499, mean_q: -46.758125\n",
      "  97000/150000: episode: 485, duration: 3.688s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.128021, mae: 31.662535, mean_q: -46.722965\n",
      "  97200/150000: episode: 486, duration: 3.655s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 2.568047, mae: 31.726347, mean_q: -46.874523\n",
      "  97400/150000: episode: 487, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.683123, mae: 31.508181, mean_q: -46.428360\n",
      "  97600/150000: episode: 488, duration: 3.638s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 4.802366, mae: 31.616575, mean_q: -46.554798\n",
      "  97800/150000: episode: 489, duration: 3.654s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.143536, mae: 31.484051, mean_q: -46.383705\n",
      "  98000/150000: episode: 490, duration: 3.724s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.615442, mae: 31.415499, mean_q: -46.303593\n",
      "  98200/150000: episode: 491, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.204365, mae: 31.035093, mean_q: -45.699677\n",
      "  98400/150000: episode: 492, duration: 3.539s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.350319, mae: 31.206158, mean_q: -45.997765\n",
      "  98600/150000: episode: 493, duration: 3.639s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.653047, mae: 30.897383, mean_q: -45.448925\n",
      "  98800/150000: episode: 494, duration: 3.628s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 4.350339, mae: 30.829823, mean_q: -45.354370\n",
      "  99000/150000: episode: 495, duration: 3.694s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.795639, mae: 30.929619, mean_q: -45.576420\n",
      "  99200/150000: episode: 496, duration: 3.695s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.421381, mae: 30.460064, mean_q: -44.794956\n",
      "  99400/150000: episode: 497, duration: 3.692s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.917972, mae: 30.322504, mean_q: -44.538242\n",
      "  99600/150000: episode: 498, duration: 3.587s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.013862, mae: 30.128740, mean_q: -44.305126\n",
      "  99800/150000: episode: 499, duration: 3.510s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.565384, mae: 29.861235, mean_q: -43.892521\n",
      " 100000/150000: episode: 500, duration: 3.595s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 2.615314, mae: 29.786831, mean_q: -43.803280\n",
      " 100200/150000: episode: 501, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.455462, mae: 29.797436, mean_q: -43.747971\n",
      " 100400/150000: episode: 502, duration: 3.608s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.465315, mae: 29.396746, mean_q: -43.202370\n",
      " 100600/150000: episode: 503, duration: 3.584s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.934772, mae: 29.235964, mean_q: -42.941246\n",
      " 100800/150000: episode: 504, duration: 3.557s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 1.771921, mae: 29.348145, mean_q: -43.154404\n",
      " 101000/150000: episode: 505, duration: 3.664s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.052967, mae: 28.797382, mean_q: -42.250206\n",
      " 101200/150000: episode: 506, duration: 3.756s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.246812, mae: 28.749540, mean_q: -42.200195\n",
      " 101400/150000: episode: 507, duration: 3.559s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 2.756880, mae: 28.391418, mean_q: -41.600586\n",
      " 101600/150000: episode: 508, duration: 3.562s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.903827, mae: 28.273767, mean_q: -41.360176\n",
      " 101800/150000: episode: 509, duration: 3.734s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 2.720901, mae: 28.052490, mean_q: -41.079327\n",
      " 101959/150000: episode: 510, duration: 2.979s, episode steps: 159, steps per second:  53, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000],  loss: 3.520010, mae: 27.656416, mean_q: -40.443398\n",
      " 102159/150000: episode: 511, duration: 3.676s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 2.793696, mae: 27.597744, mean_q: -40.430161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 102359/150000: episode: 512, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.187076, mae: 27.152021, mean_q: -39.785606\n",
      " 102559/150000: episode: 513, duration: 3.588s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.542477, mae: 27.159500, mean_q: -39.744766\n",
      " 102759/150000: episode: 514, duration: 3.565s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.190366, mae: 26.896635, mean_q: -39.388138\n",
      " 102959/150000: episode: 515, duration: 3.432s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.182590, mae: 26.759899, mean_q: -39.217407\n",
      " 103159/150000: episode: 516, duration: 3.509s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 1.491853, mae: 26.584614, mean_q: -38.938145\n",
      " 103359/150000: episode: 517, duration: 3.585s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.619216, mae: 26.422373, mean_q: -38.591320\n",
      " 103559/150000: episode: 518, duration: 3.642s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.845826, mae: 26.098862, mean_q: -38.082634\n",
      " 103759/150000: episode: 519, duration: 3.617s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.207073, mae: 25.656349, mean_q: -37.458981\n",
      " 103959/150000: episode: 520, duration: 3.663s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 1.046696, mae: 25.569012, mean_q: -37.413513\n",
      " 104159/150000: episode: 521, duration: 3.475s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.252418, mae: 25.373100, mean_q: -37.118534\n",
      " 104359/150000: episode: 522, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 2.433517, mae: 25.214842, mean_q: -36.832184\n",
      " 104559/150000: episode: 523, duration: 3.508s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 2.292694, mae: 25.003452, mean_q: -36.490997\n",
      " 104759/150000: episode: 524, duration: 3.553s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 2.563797, mae: 24.730476, mean_q: -36.039806\n",
      " 104959/150000: episode: 525, duration: 3.461s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 2.216987, mae: 24.419039, mean_q: -35.703060\n",
      " 105159/150000: episode: 526, duration: 3.623s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.541103, mae: 24.394585, mean_q: -35.704784\n",
      " 105359/150000: episode: 527, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.933784, mae: 24.319147, mean_q: -35.541447\n",
      " 105559/150000: episode: 528, duration: 3.711s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.705993, mae: 24.083618, mean_q: -35.207603\n",
      " 105759/150000: episode: 529, duration: 3.640s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.668402, mae: 24.056953, mean_q: -35.186325\n",
      " 105959/150000: episode: 530, duration: 3.504s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.499278, mae: 24.082808, mean_q: -35.268856\n",
      " 106159/150000: episode: 531, duration: 3.723s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.002317, mae: 23.940746, mean_q: -34.988384\n",
      " 106359/150000: episode: 532, duration: 3.549s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.324397, mae: 23.906137, mean_q: -35.032135\n",
      " 106559/150000: episode: 533, duration: 3.717s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.491207, mae: 23.832850, mean_q: -34.879852\n",
      " 106759/150000: episode: 534, duration: 3.753s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 2.133555, mae: 23.899185, mean_q: -34.934261\n",
      " 106959/150000: episode: 535, duration: 3.694s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.625014, mae: 23.741270, mean_q: -34.654476\n",
      " 107159/150000: episode: 536, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.856802, mae: 23.730722, mean_q: -34.686111\n",
      " 107359/150000: episode: 537, duration: 3.542s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 2.013748, mae: 23.656870, mean_q: -34.586830\n",
      " 107559/150000: episode: 538, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 2.699139, mae: 23.410488, mean_q: -34.151348\n",
      " 107759/150000: episode: 539, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.679076, mae: 23.478462, mean_q: -34.282486\n",
      " 107959/150000: episode: 540, duration: 3.599s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 2.384824, mae: 23.524649, mean_q: -34.390049\n",
      " 108159/150000: episode: 541, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.784503, mae: 23.340052, mean_q: -34.184483\n",
      " 108359/150000: episode: 542, duration: 3.586s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.385302, mae: 23.306162, mean_q: -34.177650\n",
      " 108559/150000: episode: 543, duration: 3.652s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.380986, mae: 23.506699, mean_q: -34.488659\n",
      " 108759/150000: episode: 544, duration: 3.609s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.357243, mae: 23.555796, mean_q: -34.575321\n",
      " 108959/150000: episode: 545, duration: 3.618s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 2.080439, mae: 23.661995, mean_q: -34.687023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 109159/150000: episode: 546, duration: 3.545s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.485134, mae: 23.758570, mean_q: -34.786198\n",
      " 109359/150000: episode: 547, duration: 3.653s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.190183, mae: 23.793716, mean_q: -34.826416\n",
      " 109559/150000: episode: 548, duration: 3.633s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 2.448547, mae: 23.751474, mean_q: -34.822277\n",
      " 109759/150000: episode: 549, duration: 3.650s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.748687, mae: 23.842510, mean_q: -34.982227\n",
      " 109959/150000: episode: 550, duration: 3.630s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 2.050639, mae: 23.837070, mean_q: -34.977379\n",
      " 110159/150000: episode: 551, duration: 3.757s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.937674, mae: 23.878132, mean_q: -34.975075\n",
      " 110359/150000: episode: 552, duration: 3.629s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 1.515853, mae: 24.032742, mean_q: -35.355335\n",
      " 110559/150000: episode: 553, duration: 3.678s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.271044, mae: 24.086885, mean_q: -35.446423\n",
      " 110759/150000: episode: 554, duration: 3.632s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.299592, mae: 24.147541, mean_q: -35.449791\n",
      " 110959/150000: episode: 555, duration: 3.522s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.105533, mae: 24.319927, mean_q: -35.782448\n",
      " 111159/150000: episode: 556, duration: 3.536s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 1.486924, mae: 24.516594, mean_q: -36.128529\n",
      " 111359/150000: episode: 557, duration: 3.659s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.656684, mae: 24.590103, mean_q: -36.141373\n",
      " 111559/150000: episode: 558, duration: 3.647s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.327442, mae: 24.630554, mean_q: -36.222347\n",
      " 111759/150000: episode: 559, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.363863, mae: 24.874004, mean_q: -36.630676\n",
      " 111959/150000: episode: 560, duration: 3.536s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 1.819016, mae: 24.983040, mean_q: -36.801933\n",
      " 112159/150000: episode: 561, duration: 3.647s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.997951, mae: 25.179556, mean_q: -37.062962\n",
      " 112359/150000: episode: 562, duration: 3.660s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.400688, mae: 25.381538, mean_q: -37.341339\n",
      " 112559/150000: episode: 563, duration: 3.662s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 3.537717, mae: 25.369530, mean_q: -37.253036\n",
      " 112759/150000: episode: 564, duration: 3.543s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.503098, mae: 25.440132, mean_q: -37.537941\n",
      " 112959/150000: episode: 565, duration: 3.678s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 1.962913, mae: 25.639477, mean_q: -37.825695\n",
      " 113159/150000: episode: 566, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.888981, mae: 25.853079, mean_q: -38.022961\n",
      " 113359/150000: episode: 567, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.801850, mae: 26.102680, mean_q: -38.501801\n",
      " 113559/150000: episode: 568, duration: 3.599s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 2.554255, mae: 26.215693, mean_q: -38.562012\n",
      " 113759/150000: episode: 569, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 2.749559, mae: 26.266855, mean_q: -38.682407\n",
      " 113959/150000: episode: 570, duration: 3.735s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 2.650172, mae: 26.544319, mean_q: -39.100014\n",
      " 114159/150000: episode: 571, duration: 3.667s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000],  loss: 2.859046, mae: 26.537844, mean_q: -39.069233\n",
      " 114359/150000: episode: 572, duration: 3.501s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 2.084327, mae: 26.703621, mean_q: -39.341995\n",
      " 114559/150000: episode: 573, duration: 3.492s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.570027, mae: 26.751293, mean_q: -39.355064\n",
      " 114759/150000: episode: 574, duration: 3.586s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.757860, mae: 27.066334, mean_q: -39.838215\n",
      " 114959/150000: episode: 575, duration: 3.607s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.294277, mae: 27.031723, mean_q: -39.834515\n",
      " 115159/150000: episode: 576, duration: 3.604s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 3.102072, mae: 27.204174, mean_q: -40.048908\n",
      " 115359/150000: episode: 577, duration: 3.753s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.028291, mae: 27.185575, mean_q: -40.063019\n",
      " 115559/150000: episode: 578, duration: 3.571s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 2.564619, mae: 27.413731, mean_q: -40.391834\n",
      " 115759/150000: episode: 579, duration: 3.538s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.910731, mae: 27.424219, mean_q: -40.395313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 115959/150000: episode: 580, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 1.877730, mae: 27.433331, mean_q: -40.456593\n",
      " 116159/150000: episode: 581, duration: 3.453s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 3.470891, mae: 27.664062, mean_q: -40.703228\n",
      " 116359/150000: episode: 582, duration: 3.679s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.679851, mae: 27.851154, mean_q: -40.961884\n",
      " 116559/150000: episode: 583, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.887891, mae: 27.807619, mean_q: -41.018238\n",
      " 116759/150000: episode: 584, duration: 3.497s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.794596, mae: 27.949148, mean_q: -41.188465\n",
      " 116959/150000: episode: 585, duration: 3.605s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 2.191757, mae: 28.140020, mean_q: -41.529701\n",
      " 117159/150000: episode: 586, duration: 3.456s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.597539, mae: 28.061790, mean_q: -41.322044\n",
      " 117359/150000: episode: 587, duration: 3.620s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.467429, mae: 28.292231, mean_q: -41.692013\n",
      " 117559/150000: episode: 588, duration: 3.660s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.882274, mae: 28.123306, mean_q: -41.397144\n",
      " 117759/150000: episode: 589, duration: 3.611s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 2.807842, mae: 28.376631, mean_q: -41.833431\n",
      " 117959/150000: episode: 590, duration: 3.595s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.690770, mae: 28.386402, mean_q: -41.817459\n",
      " 118159/150000: episode: 591, duration: 3.608s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.447001, mae: 28.530956, mean_q: -42.068417\n",
      " 118359/150000: episode: 592, duration: 3.540s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 2.318995, mae: 28.645435, mean_q: -42.246094\n",
      " 118559/150000: episode: 593, duration: 3.586s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.275223, mae: 28.742996, mean_q: -42.364899\n",
      " 118759/150000: episode: 594, duration: 3.721s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.927249, mae: 28.784235, mean_q: -42.434326\n",
      " 118959/150000: episode: 595, duration: 3.608s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.655150, mae: 28.688589, mean_q: -42.213364\n",
      " 119159/150000: episode: 596, duration: 3.532s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.929878, mae: 28.844414, mean_q: -42.480484\n",
      " 119359/150000: episode: 597, duration: 3.646s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.046196, mae: 28.781889, mean_q: -42.433121\n",
      " 119559/150000: episode: 598, duration: 3.640s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 2.778984, mae: 28.847240, mean_q: -42.462524\n",
      " 119759/150000: episode: 599, duration: 3.611s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.813263, mae: 29.006079, mean_q: -42.686874\n",
      " 119959/150000: episode: 600, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 2.222027, mae: 28.974401, mean_q: -42.725094\n",
      " 120159/150000: episode: 601, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.235125, mae: 29.066387, mean_q: -42.849823\n",
      " 120359/150000: episode: 602, duration: 3.558s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 2.886591, mae: 29.042051, mean_q: -42.749123\n",
      " 120559/150000: episode: 603, duration: 3.615s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.444716, mae: 29.137375, mean_q: -42.934513\n",
      " 120759/150000: episode: 604, duration: 3.741s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 2.985654, mae: 29.004663, mean_q: -42.637218\n",
      " 120959/150000: episode: 605, duration: 3.467s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.411061, mae: 28.975513, mean_q: -42.614922\n",
      " 121159/150000: episode: 606, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.283316, mae: 28.892778, mean_q: -42.524376\n",
      " 121359/150000: episode: 607, duration: 3.668s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.238839, mae: 28.755283, mean_q: -42.225868\n",
      " 121559/150000: episode: 608, duration: 3.500s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.382196, mae: 28.599848, mean_q: -42.075199\n",
      " 121759/150000: episode: 609, duration: 3.394s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.791569, mae: 28.746527, mean_q: -42.296921\n",
      " 121959/150000: episode: 610, duration: 3.492s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.219253, mae: 28.625351, mean_q: -42.153378\n",
      " 122159/150000: episode: 611, duration: 3.659s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.542019, mae: 28.567825, mean_q: -41.977745\n",
      " 122359/150000: episode: 612, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.835183, mae: 28.626909, mean_q: -42.237206\n",
      " 122559/150000: episode: 613, duration: 3.650s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.453362, mae: 28.756123, mean_q: -42.362274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 122759/150000: episode: 614, duration: 3.747s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 2.648351, mae: 28.687651, mean_q: -42.217628\n",
      " 122959/150000: episode: 615, duration: 3.587s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.494811, mae: 28.654043, mean_q: -42.207588\n",
      " 123159/150000: episode: 616, duration: 3.738s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 3.203196, mae: 28.650459, mean_q: -42.167778\n",
      " 123359/150000: episode: 617, duration: 3.588s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.384006, mae: 28.508276, mean_q: -41.989922\n",
      " 123559/150000: episode: 618, duration: 3.570s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.284926, mae: 28.495798, mean_q: -41.986759\n",
      " 123759/150000: episode: 619, duration: 3.573s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.717567, mae: 28.760414, mean_q: -42.351391\n",
      " 123959/150000: episode: 620, duration: 3.635s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.114144, mae: 28.778837, mean_q: -42.449482\n",
      " 124159/150000: episode: 621, duration: 3.680s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.574162, mae: 28.516567, mean_q: -41.966362\n",
      " 124359/150000: episode: 622, duration: 3.449s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.448816, mae: 28.764292, mean_q: -42.341381\n",
      " 124559/150000: episode: 623, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.053274, mae: 28.638943, mean_q: -42.149052\n",
      " 124759/150000: episode: 624, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.138438, mae: 28.689741, mean_q: -42.277069\n",
      " 124959/150000: episode: 625, duration: 3.619s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.567192, mae: 28.616461, mean_q: -42.159199\n",
      " 125159/150000: episode: 626, duration: 3.582s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.721845, mae: 28.586187, mean_q: -42.010033\n",
      " 125359/150000: episode: 627, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.236800, mae: 28.544058, mean_q: -42.043034\n",
      " 125559/150000: episode: 628, duration: 3.485s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.389431, mae: 28.346392, mean_q: -41.651573\n",
      " 125759/150000: episode: 629, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.449880, mae: 28.496138, mean_q: -41.961681\n",
      " 125959/150000: episode: 630, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.524912, mae: 28.224571, mean_q: -41.509274\n",
      " 126159/150000: episode: 631, duration: 3.546s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.757517, mae: 28.374622, mean_q: -41.715221\n",
      " 126359/150000: episode: 632, duration: 3.661s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.251048, mae: 28.447149, mean_q: -41.907337\n",
      " 126559/150000: episode: 633, duration: 3.511s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.896796, mae: 28.441885, mean_q: -41.753258\n",
      " 126759/150000: episode: 634, duration: 3.572s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 2.633964, mae: 28.285429, mean_q: -41.599888\n",
      " 126959/150000: episode: 635, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.669755, mae: 28.084976, mean_q: -41.247158\n",
      " 127159/150000: episode: 636, duration: 3.679s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.799088, mae: 28.023819, mean_q: -41.222870\n",
      " 127359/150000: episode: 637, duration: 3.681s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.023990, mae: 27.913261, mean_q: -41.023205\n",
      " 127559/150000: episode: 638, duration: 3.654s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.676912, mae: 27.971840, mean_q: -41.142128\n",
      " 127759/150000: episode: 639, duration: 3.475s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 2.117843, mae: 27.904512, mean_q: -41.075699\n",
      " 127959/150000: episode: 640, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.618308, mae: 27.958061, mean_q: -41.155518\n",
      " 128159/150000: episode: 641, duration: 3.463s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.430769, mae: 27.859682, mean_q: -41.014305\n",
      " 128359/150000: episode: 642, duration: 3.578s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.293875, mae: 27.943438, mean_q: -41.142746\n",
      " 128559/150000: episode: 643, duration: 3.707s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.110127, mae: 27.983837, mean_q: -41.154907\n",
      " 128759/150000: episode: 644, duration: 3.678s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.860042, mae: 27.900524, mean_q: -41.053799\n",
      " 128959/150000: episode: 645, duration: 3.672s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 1.960148, mae: 27.906240, mean_q: -41.042412\n",
      " 129159/150000: episode: 646, duration: 3.772s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.719714, mae: 27.887367, mean_q: -41.044735\n",
      " 129359/150000: episode: 647, duration: 3.634s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.454624, mae: 27.728027, mean_q: -40.741421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 129559/150000: episode: 648, duration: 3.670s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 3.206414, mae: 27.745073, mean_q: -40.791969\n",
      " 129759/150000: episode: 649, duration: 3.605s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.140985, mae: 27.747509, mean_q: -40.839123\n",
      " 129959/150000: episode: 650, duration: 3.664s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.302392, mae: 27.739817, mean_q: -40.787045\n",
      " 130159/150000: episode: 651, duration: 3.709s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.929875, mae: 27.671612, mean_q: -40.732922\n",
      " 130359/150000: episode: 652, duration: 3.670s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.962435, mae: 27.725109, mean_q: -40.739605\n",
      " 130559/150000: episode: 653, duration: 3.641s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.391655, mae: 27.794960, mean_q: -40.878513\n",
      " 130759/150000: episode: 654, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.359646, mae: 27.793308, mean_q: -40.945427\n",
      " 130959/150000: episode: 655, duration: 3.601s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 2.805100, mae: 27.872627, mean_q: -40.972076\n",
      " 131159/150000: episode: 656, duration: 3.697s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.151737, mae: 27.668547, mean_q: -40.656254\n",
      " 131359/150000: episode: 657, duration: 3.627s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.558759, mae: 27.708900, mean_q: -40.757607\n",
      " 131559/150000: episode: 658, duration: 3.735s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.701328, mae: 27.775650, mean_q: -40.803089\n",
      " 131759/150000: episode: 659, duration: 3.531s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.726908, mae: 27.645569, mean_q: -40.603325\n",
      " 131959/150000: episode: 660, duration: 3.721s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.002744, mae: 27.765696, mean_q: -40.792198\n",
      " 132159/150000: episode: 661, duration: 3.534s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 1.961909, mae: 27.543509, mean_q: -40.512367\n",
      " 132359/150000: episode: 662, duration: 3.666s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.366267, mae: 27.563683, mean_q: -40.548832\n",
      " 132559/150000: episode: 663, duration: 3.572s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.287007, mae: 27.662704, mean_q: -40.681084\n",
      " 132759/150000: episode: 664, duration: 3.650s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.601184, mae: 27.490873, mean_q: -40.399776\n",
      " 132959/150000: episode: 665, duration: 3.583s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.064159, mae: 27.572277, mean_q: -40.536102\n",
      " 133159/150000: episode: 666, duration: 3.643s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.365106, mae: 27.635681, mean_q: -40.668671\n",
      " 133359/150000: episode: 667, duration: 3.572s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.095913, mae: 27.577770, mean_q: -40.560501\n",
      " 133559/150000: episode: 668, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.841478, mae: 27.485937, mean_q: -40.341717\n",
      " 133759/150000: episode: 669, duration: 3.678s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.989502, mae: 27.523060, mean_q: -40.470745\n",
      " 133959/150000: episode: 670, duration: 3.599s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.091867, mae: 27.461348, mean_q: -40.366898\n",
      " 134159/150000: episode: 671, duration: 3.622s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 2.596081, mae: 27.354975, mean_q: -40.191002\n",
      " 134359/150000: episode: 672, duration: 3.540s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.556186, mae: 27.392378, mean_q: -40.240700\n",
      " 134559/150000: episode: 673, duration: 3.644s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.969222, mae: 27.362017, mean_q: -40.221596\n",
      " 134759/150000: episode: 674, duration: 3.665s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.783194, mae: 27.446360, mean_q: -40.336712\n",
      " 134959/150000: episode: 675, duration: 3.405s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.773964, mae: 27.235052, mean_q: -39.943039\n",
      " 135154/150000: episode: 676, duration: 3.594s, episode steps: 195, steps per second:  54, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.744767, mae: 27.211788, mean_q: -39.922882\n",
      " 135354/150000: episode: 677, duration: 3.463s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.414493, mae: 26.956078, mean_q: -39.621387\n",
      " 135554/150000: episode: 678, duration: 3.670s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.719448, mae: 27.072212, mean_q: -39.737118\n",
      " 135754/150000: episode: 679, duration: 3.434s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.528770, mae: 27.075045, mean_q: -39.787735\n",
      " 135954/150000: episode: 680, duration: 3.595s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.266167, mae: 27.003424, mean_q: -39.651821\n",
      " 136154/150000: episode: 681, duration: 3.693s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.098860, mae: 26.922523, mean_q: -39.594765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 136354/150000: episode: 682, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.034496, mae: 26.897520, mean_q: -39.488964\n",
      " 136554/150000: episode: 683, duration: 3.658s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.083821, mae: 27.029882, mean_q: -39.710655\n",
      " 136754/150000: episode: 684, duration: 3.627s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.043050, mae: 26.975658, mean_q: -39.614975\n",
      " 136954/150000: episode: 685, duration: 3.693s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 1.942770, mae: 26.993130, mean_q: -39.618088\n",
      " 137154/150000: episode: 686, duration: 3.675s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.903910, mae: 26.951820, mean_q: -39.569881\n",
      " 137354/150000: episode: 687, duration: 3.662s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.965436, mae: 26.956884, mean_q: -39.599403\n",
      " 137554/150000: episode: 688, duration: 3.574s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 2.343005, mae: 26.798916, mean_q: -39.347866\n",
      " 137754/150000: episode: 689, duration: 3.629s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.743840, mae: 26.707647, mean_q: -39.241909\n",
      " 137931/150000: episode: 690, duration: 3.195s, episode steps: 177, steps per second:  55, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.949 [0.000, 2.000],  loss: 1.588200, mae: 26.742496, mean_q: -39.301830\n",
      " 138131/150000: episode: 691, duration: 3.464s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 2.295184, mae: 26.827347, mean_q: -39.409668\n",
      " 138331/150000: episode: 692, duration: 3.636s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.296832, mae: 26.681458, mean_q: -39.265625\n",
      " 138531/150000: episode: 693, duration: 3.546s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.669779, mae: 26.892090, mean_q: -39.498028\n",
      " 138731/150000: episode: 694, duration: 3.734s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.400644, mae: 26.784901, mean_q: -39.393276\n",
      " 138931/150000: episode: 695, duration: 3.554s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.967985, mae: 26.912470, mean_q: -39.528221\n",
      " 139131/150000: episode: 696, duration: 3.600s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 1.644407, mae: 26.699160, mean_q: -39.245682\n",
      " 139331/150000: episode: 697, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.729446, mae: 26.971220, mean_q: -39.668026\n",
      " 139531/150000: episode: 698, duration: 3.538s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.378116, mae: 26.916166, mean_q: -39.608704\n",
      " 139731/150000: episode: 699, duration: 3.530s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 1.710413, mae: 26.805063, mean_q: -39.408585\n",
      " 139931/150000: episode: 700, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.622699, mae: 26.933832, mean_q: -39.637840\n",
      " 140131/150000: episode: 701, duration: 3.683s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.670696, mae: 26.912653, mean_q: -39.602901\n",
      " 140331/150000: episode: 702, duration: 3.576s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 1.812558, mae: 27.036396, mean_q: -39.777050\n",
      " 140531/150000: episode: 703, duration: 3.553s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 1.472084, mae: 27.149071, mean_q: -39.962963\n",
      " 140731/150000: episode: 704, duration: 3.646s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.446856, mae: 27.122604, mean_q: -39.941357\n",
      " 140931/150000: episode: 705, duration: 3.667s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.935782, mae: 26.901117, mean_q: -39.570057\n",
      " 141131/150000: episode: 706, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 2.090230, mae: 27.147285, mean_q: -39.867100\n",
      " 141331/150000: episode: 707, duration: 3.668s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.563110, mae: 27.026684, mean_q: -39.708542\n",
      " 141531/150000: episode: 708, duration: 3.686s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 1.901756, mae: 27.090601, mean_q: -39.792015\n",
      " 141731/150000: episode: 709, duration: 3.682s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.101807, mae: 27.197803, mean_q: -40.066864\n",
      " 141931/150000: episode: 710, duration: 3.671s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.350356, mae: 27.141699, mean_q: -39.968033\n",
      " 142131/150000: episode: 711, duration: 3.629s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.292889, mae: 27.294300, mean_q: -40.188446\n",
      " 142331/150000: episode: 712, duration: 3.773s, episode steps: 200, steps per second:  53, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.787389, mae: 27.177805, mean_q: -39.892853\n",
      " 142531/150000: episode: 713, duration: 3.682s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.141026, mae: 27.125648, mean_q: -39.820938\n",
      " 142731/150000: episode: 714, duration: 3.435s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.933342, mae: 27.077326, mean_q: -39.798401\n",
      " 142931/150000: episode: 715, duration: 3.641s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.910775, mae: 27.113262, mean_q: -39.883034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143131/150000: episode: 716, duration: 3.704s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.852261, mae: 27.201763, mean_q: -39.980240\n",
      " 143331/150000: episode: 717, duration: 3.704s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.855474, mae: 27.209457, mean_q: -40.017262\n",
      " 143531/150000: episode: 718, duration: 3.678s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.470989, mae: 27.217056, mean_q: -40.029129\n",
      " 143731/150000: episode: 719, duration: 3.581s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.362884, mae: 27.279579, mean_q: -40.099075\n",
      " 143931/150000: episode: 720, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.866485, mae: 27.058622, mean_q: -39.764214\n",
      " 144131/150000: episode: 721, duration: 3.603s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.116727, mae: 27.110170, mean_q: -39.878063\n",
      " 144331/150000: episode: 722, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.566955, mae: 27.160557, mean_q: -39.902611\n",
      " 144531/150000: episode: 723, duration: 3.655s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.595825, mae: 27.137486, mean_q: -39.864471\n",
      " 144731/150000: episode: 724, duration: 3.635s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.135645, mae: 27.173609, mean_q: -39.888241\n",
      " 144931/150000: episode: 725, duration: 3.663s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.114074, mae: 27.026949, mean_q: -39.790699\n",
      " 145131/150000: episode: 726, duration: 3.581s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.843014, mae: 27.081081, mean_q: -39.793026\n",
      " 145331/150000: episode: 727, duration: 3.612s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.730136, mae: 27.036057, mean_q: -39.714550\n",
      " 145531/150000: episode: 728, duration: 3.716s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 1.413444, mae: 27.051397, mean_q: -39.775780\n",
      " 145731/150000: episode: 729, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.578283, mae: 27.294931, mean_q: -40.110195\n",
      " 145931/150000: episode: 730, duration: 3.697s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.266537, mae: 27.024033, mean_q: -39.721771\n",
      " 146131/150000: episode: 731, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.679976, mae: 26.803457, mean_q: -39.367409\n",
      " 146331/150000: episode: 732, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 1.623303, mae: 26.828392, mean_q: -39.417007\n",
      " 146531/150000: episode: 733, duration: 3.598s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 1.038094, mae: 26.785860, mean_q: -39.379337\n",
      " 146731/150000: episode: 734, duration: 3.432s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.380607, mae: 27.093985, mean_q: -39.867802\n",
      " 146931/150000: episode: 735, duration: 3.596s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.385885, mae: 26.953701, mean_q: -39.579681\n",
      " 147131/150000: episode: 736, duration: 3.600s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 1.342292, mae: 26.854918, mean_q: -39.459793\n",
      " 147331/150000: episode: 737, duration: 3.552s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.822825, mae: 27.213980, mean_q: -39.921459\n",
      " 147531/150000: episode: 738, duration: 3.689s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.487457, mae: 27.114038, mean_q: -39.830151\n",
      " 147731/150000: episode: 739, duration: 3.540s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.627148, mae: 26.855556, mean_q: -39.406570\n",
      " 147931/150000: episode: 740, duration: 3.656s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.828799, mae: 26.800667, mean_q: -39.285385\n",
      " 148131/150000: episode: 741, duration: 3.624s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.762235, mae: 26.680498, mean_q: -39.209732\n",
      " 148331/150000: episode: 742, duration: 3.531s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.360158, mae: 26.799473, mean_q: -39.337452\n",
      " 148531/150000: episode: 743, duration: 3.540s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 1.121896, mae: 26.650799, mean_q: -39.114639\n",
      " 148731/150000: episode: 744, duration: 3.531s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.505355, mae: 26.572060, mean_q: -38.957386\n",
      " 148931/150000: episode: 745, duration: 3.677s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.198580, mae: 26.700233, mean_q: -39.186710\n",
      " 149131/150000: episode: 746, duration: 3.631s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.324797, mae: 26.538956, mean_q: -38.988880\n",
      " 149331/150000: episode: 747, duration: 3.667s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.919193, mae: 26.639782, mean_q: -39.179649\n",
      " 149531/150000: episode: 748, duration: 3.635s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.440960, mae: 26.676031, mean_q: -39.133698\n",
      " 149731/150000: episode: 749, duration: 3.632s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.535454, mae: 26.625507, mean_q: -39.041660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 149931/150000: episode: 750, duration: 3.651s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.326276, mae: 26.500669, mean_q: -38.882103\n",
      "done, took 2728.478 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89de775790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('results/dqn_mountaincar_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -116.000, steps: 116\n",
      "Episode 2: reward: -172.000, steps: 172\n",
      "Episode 3: reward: -162.000, steps: 162\n",
      "Episode 4: reward: -117.000, steps: 117\n",
      "Episode 5: reward: -156.000, steps: 156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8abb3d8c50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
